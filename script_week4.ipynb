{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "53221393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # for load from env \n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "import re # regex module\n",
    "import logging\n",
    "\n",
    "from minio import Minio \n",
    "from io  import BytesIO\n",
    "\n",
    "from sqlalchemy import create_engine \n",
    "import sqlalchemy \n",
    "from pangres import upsert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e83c6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load env variables \n",
    "load_dotenv(\".env\")\n",
    "\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_SCHEMA_STG = os.getenv(\"DB_SCHEMA_STG\")\n",
    "DB_SCHEMA_LOG = os.getenv(\"DB_SCHEMA_LOG\")\n",
    "DB_SCHEMA_DWH = os.getenv(\"DB_SCHEMA_DWH\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\")   \n",
    "\n",
    "# get minio access from env \n",
    "ACCESS_KEY_MINIO = os.getenv(\"ACCESS_KEY_MINIO\")\n",
    "SECRET_KEY_MINIO = os.getenv(\"SECRET_KEY_MINIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d55f4",
   "metadata": {},
   "source": [
    "#### Read SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "18f230f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql(table_name) :\n",
    "    # open sql file and read content\n",
    "    with open(f\"{MODEL_PATH}{table_name}.sql\",\"r\") as file:\n",
    "        content = file.read()\n",
    "    # return to text query\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152d7dd",
   "metadata": {},
   "source": [
    "#### ETL Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5a33ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_log(log_msg: dict) :\n",
    "    try :\n",
    "        #create connection database \n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "\n",
    "        # change dictionary log_msg to dataframe\n",
    "        df_log = pd.DataFrame([log_msg])\n",
    "\n",
    "        #extract data log \n",
    "        with conn.connect() as connection :\n",
    "            df_log.to_sql(\n",
    "                name = \"etl_log\",\n",
    "                con = connection,\n",
    "                schema = \"log\",\n",
    "                if_exists = \"append\",\n",
    "                index = False\n",
    "            )\n",
    "    except Exception as e :\n",
    "        logging.error(f\"Cant save your log message. Error: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735afb71",
   "metadata": {},
   "source": [
    "#### Read ETL Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2fccfd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_etl_log(filter_params: dict) :\n",
    "    try :\n",
    "        # create connection to database \n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "\n",
    "        # get etl_date from latest\n",
    "        query = sqlalchemy.text(read_sql(\"log\"))\n",
    "\n",
    "        # execute query with pd.read_sql \n",
    "        with conn.connect() as connection :\n",
    "            df = pd.read_sql(\n",
    "                sql = query,\n",
    "                con = connection,\n",
    "                params = (filter_params,)\n",
    "            )\n",
    "        # return extracted data \n",
    "        return df\n",
    "    except Exception as  e :\n",
    "        logging.error(f\"Cant execute your query. Error: {e}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a77a0",
   "metadata": {},
   "source": [
    "#### Extract Staging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88bc7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_staging(table_name:str, schema_name:str) -> pd.DataFrame :\n",
    "    try :\n",
    "        # create connection to database \n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "\n",
    "        # get date from previous process in etl_log \n",
    "        filter_log = {\n",
    "            \"step_name\" : \"warehouse\",\n",
    "            \"table_name\" : table_name,\n",
    "            \"status\" : \"success\",\n",
    "            \"process\" : \"load\"\n",
    "        }\n",
    "        etl_date = read_etl_log(filter_log) \n",
    "\n",
    "        # if previous process is null, set etl_date to 1990-01-01\n",
    "        # if previous process is not null, get the latest etl_date \n",
    "        if (etl_date['max'][0] == None) :\n",
    "            etl_date = '1990-01-01'\n",
    "        else :\n",
    "            etl_date = etl_date[max][0]\n",
    "\n",
    "        # create query to select all column from specified table where created_at > etl_date\n",
    "        query = f\"SELECT * FROM {schema_name}.{table_name} WHERE created_at > %s::timestamp\"\n",
    "\n",
    "        # execute the query with pd.read_sql \n",
    "        with conn.connect() as connection :\n",
    "            df = pd.read_sql(\n",
    "                sql=query,\n",
    "                con=connection,\n",
    "                params=(etl_date,)\n",
    "            )\n",
    "        log_msg = {\n",
    "            \"step\"  : \"warehouse\",\n",
    "            \"process\" : \"extraction\",\n",
    "            \"status\" : \"success\",\n",
    "            \"source\" :  \"database\",\n",
    "            \"table_name\" : table_name, \n",
    "            \"etl_date\" : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        return df\n",
    "        \n",
    "    except Exception as  e :\n",
    "        log_msg = {\n",
    "            \"step\"  : \"warehouse\",\n",
    "            \"process\" : \"extraction\",\n",
    "            \"status\" : \"failed\",\n",
    "            \"source\" :  \"database\",\n",
    "            \"table_name\" : table_name, \n",
    "            \"etl_date\" : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        logging.error(f\"Cant execute your query. Error: {e}\", exc_info=True)\n",
    "        raise\n",
    "    finally :\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e47d2",
   "metadata": {},
   "source": [
    "#### Handle Error MINIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a710276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to handle error data and upload it to minio\n",
    "def handle_error(data, bucket_name: str, table_name: str, process: str) :\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    try : \n",
    "        client = Minio('localhost:9000',\n",
    "                       access_key=ACCESS_KEY_MINIO,\n",
    "                       secret_key=SECRET_KEY_MINIO,\n",
    "                       secure=False)\n",
    "        # check if bucket exists, if not exists then create it\n",
    "        if not client.bucket_exists(bucket_name):\n",
    "            client.make_bucket(bucket_name)\n",
    "        \n",
    "        # convert dataframe to csv and then to bytes\n",
    "        csv_bytes = data.to_csv().encode('utf-8')\n",
    "        csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "        # upload the csv file to the bucket \n",
    "        client.put_object(\n",
    "            bucket_name=bucket_name,\n",
    "            object_name=f\"{process}_{table_name}_{current_date}.csv\",\n",
    "            data=csv_buffer,\n",
    "            length=len(csv_bytes),\n",
    "            content_type='application/csv'\n",
    "        )\n",
    "\n",
    "        # list objects in the bucket \n",
    "        objects = client.list_objects(bucket_name, recursive=True)\n",
    "        for obj in objects:\n",
    "            logging.info(f\"Object in bucket: {obj.object_name}\")\n",
    "    except Exception as e :\n",
    "        logging.error(f\"Failed to upload error data to Minio. Error: {e}\", exc_info=True)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d1f4e",
   "metadata": {},
   "source": [
    "#### Load Warehouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8d45658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for load data to data warehouse\n",
    "def load_warehouse(data, schema:str, table_name:str, idx_name:str, source):\n",
    "    try :\n",
    "        # create connection to database \n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "\n",
    "        # set data index or primary key\n",
    "        data = data.set_index(idx_name)\n",
    "\n",
    "        # do upsert ( insert for non existing data, update for existing data )\n",
    "        # with conn.connect() as connection :\n",
    "        upsert(\n",
    "            df = data,\n",
    "            con = conn,\n",
    "            table_name = table_name,\n",
    "            schema = schema,\n",
    "            if_row_exists = \"update\"\n",
    "        )\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\":\"load\",\n",
    "            \"status\": \"success\",\n",
    "            \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\":\"load\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "\n",
    "        }\n",
    "        handle_error(data, bucket_name=\"error-dellstore\", table_name=table_name, process=\"load\")\n",
    "    finally :\n",
    "        etl_log(log_msg)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ae41e",
   "metadata": {},
   "source": [
    "#### Extract Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1c105467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to get data from data warehouse to obtain value of foreign key\n",
    "def extract_target(table_name:str) :\n",
    "    conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "    query = f\"SELECT * FROM public.{table_name}\"\n",
    "    with conn.connect() as connection :\n",
    "        df = pd.read_sql(\n",
    "            sql = query,\n",
    "            con = connection    \n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbfb34",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97688122",
   "metadata": {},
   "source": [
    "#### Transform Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e77062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table category from staging to data warehouse\n",
    "## - rename column from category to category_nk\n",
    "## - rename column from categoryname to category_name\n",
    "\n",
    "def transform_categories(data: pd.DataFrame, table_name:str) -> pd.DataFrame :\n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "        data = data.rename(columns={'category' : 'category_nk',\n",
    "                                   'categoryname' : 'category_name'})\n",
    "        \n",
    "        # remove dupicate based on category_nk and category_name \n",
    "        data = data.drop_duplicates(subset='category_nk')\n",
    "\n",
    "        # drop column created_at \n",
    "        data = data.drop(columns=['created_at'])\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"success\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        handle_error(data = data, \n",
    "                     bucket_name = 'error-dellstore',\n",
    "                     table_name = table_name, \n",
    "                     process = process)\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae259777",
   "metadata": {},
   "source": [
    "#### Transform Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7273687",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table customer from staging to data warehouse\n",
    "## - rename column from creditcardtype to credit_card_type\n",
    "## - rename column from creditcard to credit_card\n",
    "## - rename column from creditcardexplanation to credit_card_explanation\n",
    "## - rename column from firstname to first_name \n",
    "## - rename column from lastname to last_name\n",
    "\n",
    "def transform_customer(data: pd.DataFrame, table_name:str) -> pd.DataFrame :\n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "        data = data.rename(columns={'customerid': 'customer_nk',\n",
    "                            'firstname': 'first_name',\n",
    "                            'lastname': 'last_name',\n",
    "                            'creditcardtype': 'credit_card_type',\n",
    "                            'creditcard': 'credit_card',\n",
    "                            'creditcardexpiration': 'credit_card_expiration'})\n",
    "\n",
    "        # remove dupicate based on customer_nk and customer_name \n",
    "        data = data.drop_duplicates(subset='customer_nk')\n",
    "\n",
    "        # masking credit_card_number \n",
    "        data['credit_card'] = data['credit_card'].apply(lambda x: re.sub(r'\\d', 'X', x[:-4]) + x[-4:])\n",
    "\n",
    "        # drop column created_at \n",
    "        data = data.drop(columns=['created_at'])\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"success\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        handle_error(data = data, \n",
    "                     bucket_name = 'error-dellstore',\n",
    "                     table_name = table_name, \n",
    "                     process = process)\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b56a4",
   "metadata": {},
   "source": [
    "#### Transform product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e0a05bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table products from staging to data warehouse\n",
    "## - rename column from prod_id to product_nk\n",
    "## - rename column from category to category_nk\n",
    "## - lookup category_nk from categories tables based on category\n",
    "def transform_product(data: pd.DataFrame, table_name:str) -> pd.DataFrame :\n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "\n",
    "        #rename column product \n",
    "        data = data.rename(columns= {\n",
    "            \"prod_id\" : \"product_nk\",\n",
    "            \"category\" : \"category_nk\"\n",
    "        })\n",
    "\n",
    "        # remove duplicate based on product_nk \n",
    "        data = data.drop_duplicates(subset='product_nk')\n",
    "\n",
    "        # extract data from the 'categories' table \n",
    "        categories = extract_target('categories')\n",
    "\n",
    "        # lookup 'category_id' from categories table based on 'category'\n",
    "        data['category_id'] = data['category_nk'].apply(lambda x: categories.loc[categories['category_nk']== x, 'category_id'].values[0])\n",
    "        # category_mapping = categories.set_index('category_nk')['category_id']\n",
    "        # data['category_id'] = data['category_nk'].map(category_mapping)\n",
    "\n",
    "        # drop column created_at\n",
    "        data = data.drop(columns=['created_at','category_nk'])\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\" : process, \n",
    "            \"status\" : \"success\",\n",
    "            \"source\" : \"staging\",\n",
    "            \"table_name\"  : table_name, \n",
    "            \"etl_date\" : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "\n",
    "        return data \n",
    "    \n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\" : process, \n",
    "            \"status\" : \"failed\",\n",
    "            \"source\" : \"staging\",\n",
    "            \"table_name\"  : table_name, \n",
    "            \"etl_date\" : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        handle_error(data = data, \n",
    "                     bucket_name = 'error-dellstore',\n",
    "                     table_name = table_name, \n",
    "                     process = process)    \n",
    "    finally :\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f35336",
   "metadata": {},
   "source": [
    "#### Transform Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "25a7a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table inventory from staging to data warehouse\n",
    "## - rename column quan_in_stock to quantity_in_stock\n",
    "## - column product_id refers to products table using column product_nk\n",
    "\n",
    "def transform_inventory(data: pd.DataFrame, table_name: str) -> pd.DataFrame : \n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "\n",
    "        # rename column inventory\n",
    "        data = data.rename(columns={\n",
    "            \"quan_in_stock\" : \"quantity_stock\",\n",
    "            \"prod_id\" : \"product_nk\"\n",
    "        }) \n",
    "\n",
    "        # remove duplicate based on product_nk \n",
    "        data = data.drop_duplicates(subset='product_nk')\n",
    "\n",
    "        #extract data from the 'products' table \n",
    "        products = extract_target('products')\n",
    "\n",
    "        # lookup 'product_id' from products table based on 'product_nk'\n",
    "        data['product_id'] = data['product_nk'].apply(lambda x : products.loc[products['product_nk']== x, 'product_id'].values[0])\n",
    "        # coba pake map \n",
    "        # products_mapping = products.set_index('product_nk')['product_id']\n",
    "        # data['product_id'] = data['product_nk'].map(products_mapping)\n",
    "\n",
    "        # drop column created_at \n",
    "        data = data.drop(columns=['created_at'])\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"success\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "        return data\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        handle_error(data = data, \n",
    "                     bucket_name = 'error-dellstore',\n",
    "                     table_name = table_name, \n",
    "                     process = process)\n",
    "    finally :\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ea492",
   "metadata": {},
   "source": [
    "#### Transform Orders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5b8dc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table inventory from staging to data warehouse\n",
    "## - rename column orderid to order_nk \n",
    "## - get customer_id values from customer table by matching customer_nk \n",
    "## - rename orderdate to order_date \n",
    "## - rename netamount to net_amount \n",
    "## - rename totalamount to total_amount \n",
    "\n",
    "def transform_orders(data: pd.DataFrame, table_name: str) -> pd.DataFrame : \n",
    "    try :\n",
    "        \n",
    "        process = \"transformation\"\n",
    "\n",
    "        # rename column \n",
    "        data = data.rename(columns = {\n",
    "            \"orderid\" : \"order_nk\",\n",
    "            \"orderdate\" : \"order_date\",\n",
    "            \"netamount\" : \"net_amount\",\n",
    "            \"totalamount\" : \"total_amount\",\n",
    "            \"customerid\" : \"customer_nk\"\n",
    "        })\n",
    "\n",
    "        # extract table customer \n",
    "        customers = extract_target(\"customers\")\n",
    "\n",
    "        # lookup customer_id from customer table based on customer_nk\n",
    "        data['customer_id'] = data['customer_nk'].apply(lambda x : \n",
    "                                                        customers.loc[customers['customer_nk']==x, 'customer_id'].values[0])\n",
    "        # or using map\n",
    "        # customer_mapping = customers.set_index('customer_nk')['customer_id']  \n",
    "        # data['customer_id'] = data['customer_nk'].map(customer_mapping)\n",
    "\n",
    "        # drop column created at and customer_nk (for identifier customer can use customer_id column)\n",
    "        data = data.drop(columns = ['created_at', 'customer_nk'])\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        return data\n",
    "\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\" : str(e)\n",
    "                }\n",
    "        handle_error(\n",
    "            data=data,\n",
    "            bucket_name='error-dellstore',\n",
    "            table_name = table_name,\n",
    "            process=process\n",
    "        )\n",
    "\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824bbd0",
   "metadata": {},
   "source": [
    "#### Transform Orderline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "334177bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transform orders table from staging to data warehouse \n",
    "## - rename column orderlineid to orderline_nk\n",
    "## - rename column orderid to order_id and lookup from orders table based on orderid\n",
    "## - rename column prod_id to product_id and lookup from products based on prod_id\n",
    "## - rename column orderdate to order_date \n",
    "\n",
    "def transform_orderlines(data: pd.DataFrame, table_name: str) -> pd.DataFrame :\n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "        \n",
    "        #rename column \n",
    "        data = data.rename(columns= {\n",
    "            \"orderlineid\" : \"orderline_nk\",\n",
    "            \"orderid\" : \"order_nk\",\n",
    "            \"prod_id\" : \"product_nk\",\n",
    "            \"orderdate\"  :\"order_date\"\n",
    "        })\n",
    "        # print(data)\n",
    "\n",
    "        # extract data from orders table \n",
    "        orders = extract_target(\"orders\")\n",
    "        \n",
    "        #lookup 'order_id' \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        #or using map \n",
    "        # orders_mapping = orders.set_index('order_nk')['order_id']\n",
    "        # data['order_id'] = data['order_nk'].map(orders_mapping)\n",
    "\n",
    "        # extract data from products table \n",
    "        products = extract_target(\"products\")\n",
    "\n",
    "        #lookup product id \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: products.loc[products['product_nk'] == x, 'product_id'].values[0])\n",
    "        #or using map \n",
    "        # products_mapping = products.set_index('product_nk')['product_id']\n",
    "        # data['product_id'] = data['product_nk'].map(products_mapping)\n",
    "\n",
    "        # drop unnecessary columns \n",
    "        data = data.drop(columns = ['created_at', 'order_nk', 'product_nk'])\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        \n",
    "        handle_error(\n",
    "            data= data,\n",
    "            bucket_name= 'error-dellstore',\n",
    "            table_name = table_name,\n",
    "            process= process\n",
    "        )\n",
    "\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b482f3",
   "metadata": {},
   "source": [
    "#### Transform customer_orders_history\n",
    "\n",
    "Target:\n",
    "- Table customers\n",
    "- Table product\n",
    "- Table orders\n",
    "- Table orderlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "67b7e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_cust(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column for customers\n",
    "        data = data.rename(columns={\n",
    "                                'customer_id': 'customer_nk',\n",
    "                                'customer_firstname': 'first_name',\n",
    "                                'customer_lastname': 'last_name',\n",
    "                                'customer_address1': 'address1',\n",
    "                                'customer_address2': 'address2',\n",
    "                                'customer_city': 'city',\n",
    "                                'customer_state': 'state',\n",
    "                                'customer_zip': 'zip',\n",
    "                                'customer_country': 'country',\n",
    "                                'customer_region': 'region',\n",
    "                                'customer_email': 'email',\n",
    "                                'customer_phone': 'phone',\n",
    "                                'customer_creditcardtype': 'credit_card_type',\n",
    "                                'customer_creditcard': 'credit_card',\n",
    "                                'customer_creditcardexpiration': 'credit_card_expiration',\n",
    "                                'customer_username': 'username',\n",
    "                                'customer_password': 'password',\n",
    "                                'customer_age': 'age',\n",
    "                                'customer_income': 'income',\n",
    "                                'customer_gender': 'gender'\n",
    "                            }) \n",
    "        \n",
    "        columns_to_keep = [\n",
    "            'customer_nk', 'customer_id', 'first_name', 'last_name', \n",
    "            'address1', 'address2', 'city', 'state', 'zip', \n",
    "            'country', 'region', 'email', 'phone', \n",
    "            'credit_card_type', 'credit_card', 'credit_card_expiration', \n",
    "            'username', 'password', 'age', 'income', 'gender'\n",
    "        ]\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=[col for col in data.columns if col not in columns_to_keep])\n",
    "\n",
    "        # Deduplication based on customer_nk\n",
    "        data = data.drop_duplicates(subset='customer_nk')\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "aca59367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_prod(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data orders from customer_order_hist staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column for products\n",
    "        data = data.rename(columns={\n",
    "            'product_id': 'product_nk', \n",
    "            'product_category': 'category_nk', \n",
    "            'product_title': 'title', \n",
    "            'product_actor': 'actor', \n",
    "            'product_price': 'price', \n",
    "            'product_special': 'special', \n",
    "            'product_common_prod_id': 'common_prod_id'\n",
    "        })\n",
    "\n",
    "        # Deduplication based on product_nk\n",
    "        data = data.drop_duplicates(subset='product_nk')\n",
    "\n",
    "        # Extract data from the `categories` table\n",
    "        categories = extract_target('categories')\n",
    "\n",
    "        #Lookup `category_id` from `categories` table based on `category`   \n",
    "        data['category_id'] = data['category_nk'].apply(lambda x: categories.loc[categories['category_nk'] == x, 'category_id'].values[0])\n",
    "        \n",
    "        # Get relevant columns\n",
    "        data = data[['product_nk', 'category_id', 'title', 'actor', 'price', 'special', 'common_prod_id']]\n",
    "\n",
    "\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1b8bae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_order(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data order from customer_order_hist staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column for orders\n",
    "        data = data.rename(columns={\n",
    "                    'order_id': 'order_nk', \n",
    "                    'order_customerid': 'customer_nk', \n",
    "                    'order_date': 'order_date', \n",
    "                    'order_netamount': 'net_amount', \n",
    "                    'order_tax': 'tax', \n",
    "                    'order_totalamount': 'total_amount'\n",
    "                })\n",
    "\n",
    "\n",
    "        # Deduplication based on order_nk\n",
    "        data = data.drop_duplicates(subset='order_nk')\n",
    "\n",
    "        # Extract data from the `customers` table\n",
    "        customer = extract_target('customers')\n",
    "\n",
    "        #Lookup `customer_id` from `customers` table based on `customer_nk`   \n",
    "        data['customer_id'] = data['customer_nk'].apply(lambda x: customer.loc[customer['customer_nk'] == x, 'customer_id'].values[0])\n",
    "        \n",
    "        # Get relevant columns\n",
    "        data = data[['order_nk', 'customer_id', 'order_date', 'net_amount', 'tax', 'total_amount']]\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "701e9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_orderline(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data orderline from customer_order_hist staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        #drop column order_date\n",
    "        data = data.drop(columns=['order_date'])\n",
    "\n",
    "        # rename column for orders\n",
    "        data = data.rename(columns={\n",
    "            'orderline_id': 'orderline_nk', \n",
    "            'order_id': 'order_nk', \n",
    "            'product_id': 'product_nk', \n",
    "            'orderline_quantity': 'quantity', \n",
    "            'orderline_orderdate': 'order_date'\n",
    "        })\n",
    "\n",
    "        # Deduplication based on order_nk\n",
    "        data = data.drop_duplicates(subset=['orderline_nk','order_nk','product_nk','quantity'])\n",
    "\n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # Extract data from the `product` table\n",
    "        products = extract_target('products')\n",
    "\n",
    "        # Lookup `product_id` from `product` table based on `prod_id`   \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: products.loc[products['product_nk'] == x, 'product_id'].values[0])\n",
    "        \n",
    "        \n",
    "        # Get relevant columns\n",
    "        data = data[['orderline_nk', 'order_id', 'product_id', 'quantity', 'order_date']]\n",
    "        \n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55711a",
   "metadata": {},
   "source": [
    "#### Transform Customer History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cc80cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cust_hist(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column orderlines\n",
    "        data = data.rename(columns={'customerid':'customer_nk', \n",
    "                                    'prod_id':'product_nk', \n",
    "                                    'orderid':'order_nk'})\n",
    "        \n",
    "        # Extract data from the `customers` table\n",
    "        customers = extract_target('customers')\n",
    "\n",
    "        # Lookup `customer_id` from `customers` table based on `customerid`   \n",
    "        data['customer_id'] = data['customer_nk'].apply(lambda x: customers.loc[customers['customer_nk'] == x, 'customer_id'].values[0])\n",
    "        \n",
    "\n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # Extract data from the `product` table\n",
    "        products = extract_target('products')\n",
    "\n",
    "        # Lookup `product_id` from `product` table based on `prod_id`   \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: products.loc[products['product_nk'] == x, 'product_id'].values[0])\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        data = data.drop(columns=['customer_nk','order_nk','product_nk'])\n",
    "\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29064e4",
   "metadata": {},
   "source": [
    "#### Transform order status analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ebc80a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_status_analytic(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column order_status_analytic\n",
    "        data = data.rename(columns={'orderid':'order_nk'})\n",
    "\n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        data = data.drop(columns='created_at')\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ccb94c",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb3011b",
   "metadata": {},
   "source": [
    "#### Table Customer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e8001",
   "metadata": {},
   "source": [
    "- Validasi format email\n",
    "- Check nomor hp memiliki 10 digit\n",
    "- Check expired CC pake format YYYY/MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "63effd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_email_format(email) :\n",
    "    email_regex = re.compile(r\"^[\\w\\.-]+@(yahoo\\.com|hotmail\\.com|gmail\\.com)$\")\n",
    "    return bool(email_regex.match(email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "028cc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_phone_format(phone) :\n",
    "    phone_regex = re.compile(r\"^\\d{10}$\")\n",
    "    return bool(phone_regex.match(phone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "766dc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_credit_card_expiration_format(expiration_date) :\n",
    "    expiration_date_regex = re.compile(r\"^\\d{4}/\\d{2}$\")\n",
    "    return bool(expiration_date_regex.match(expiration_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f802739",
   "metadata": {},
   "source": [
    "#### Table Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19972bb3",
   "metadata": {},
   "source": [
    "- Check harga apakah masih mencakup range 1 - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "421fd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_price_range(price) :\n",
    "    return 0<= price <= 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1882d6c",
   "metadata": {},
   "source": [
    "#### Table Orders dan Orderline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78661a",
   "metadata": {},
   "source": [
    "table order :\n",
    "- Check apakah kolom net_amount, tax dan total_amount memiliki nilai positif\n",
    "\n",
    "table orderline :\n",
    "- Check apakah kolom quantity memiliki nomor positif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1f1468ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_positive_values(value) :\n",
    "    return value >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b582be",
   "metadata": {},
   "source": [
    "#### Table Order Status Analytic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bc7d0",
   "metadata": {},
   "source": [
    "- check status apakah partial, fulfilled atau backordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "97a866a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_order_status(status) :\n",
    "    return status in ['partial', \n",
    "                      'fulfilled',\n",
    "                      'backordered']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27210b7b",
   "metadata": {},
   "source": [
    "#### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "40051849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_data(data: pd.DataFrame, table_name: str, validation_functions: dict) -> pd.DataFrame :\n",
    "    try :\n",
    "        # create report dataframe\n",
    "        report_data = {f'validate_{name}' : data[name].apply(func) for name, func in validation_functions.items()}\n",
    "        report_df = pd.DataFrame(report_data)\n",
    "\n",
    "        # summarize status data by all conditions\n",
    "        report_df['all_valid'] = report_df.all(axis=1)\n",
    "\n",
    "        # # filter out valid rows (all_valid = 'True')\n",
    "        valid_data_df = data[report_df['all_valid']]\n",
    "\n",
    "        # # filter out invalid rows (all_valid = 'False)\n",
    "        invalid_data_df = data[~report_df['all_valid']]\n",
    "\n",
    "        # # create success log \n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\" : \"validation\",\n",
    "            \"status\" : \"success\",\n",
    "            \"source\" : \"staging\",\n",
    "            \"table_name\" : table_name, \n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        # print(invalid_data_df, valid_data_df)\n",
    "        return valid_data_df, invalid_data_df\n",
    "        # return invalid_data_df\n",
    "    except Exception as e:\n",
    "        # create fail log msg \n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\" : \"validation\",\n",
    "            \"status\" : \"success\",\n",
    "            \"source\" : \"staging\",\n",
    "            \"table_name\" : table_name, \n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\" : str(3)\n",
    "        }\n",
    "        # print(e)\n",
    "\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2576340",
   "metadata": {},
   "source": [
    "## Pipeline Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952a234",
   "metadata": {},
   "source": [
    "#### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "acb6eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text function validate \n",
    "# data = pd.DataFrame({\n",
    "#     \"customerid\" : [11,12,13,14,15], \n",
    "#     \"firstname\" : ['Becky', 'Raymond', 'Melanie', 'Heather', 'Heather'],\n",
    "#     \"email\" : [\"beckycochran@yahoo.com\",\"raymondyang@yahoo.com\",\"melaniewade@yahoo.com\",\"heathercruz@hotmail.com\",\"heatherburgess@hotmail.com\"],\n",
    "#     \"phone\" : [\"2415449050\",\"1896033667\",\"3029418206\",\"3748672054\",\"3354132892\"],\n",
    "#     \"credit_card_expiration\" : [\"2010/03\",\"2011/10\",\"2009/11\",\"2011/07\",\"2008/05\"]\n",
    "# })\n",
    "\n",
    "# print(data)\n",
    "# validation_cust, invalid_cust = validation_data(data=data, table_name='customers', validation_functions = {\"email\": validate_email_format,\n",
    "#                                                                                             \"phone\" : validate_phone_format,\n",
    "#                                                                                             \"credit_card_expiration\" : validate_credit_card_expiration_format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a6ff3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract staging\n",
    "df_category = extract_staging(table_name = \"categories\", schema_name=DB_SCHEMA_STG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1c602ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categories\n",
    "category_tf = transform_categories(data = df_category, table_name=\"categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e4e33f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_warehouse(data= category_tf, schema=\"public\", table_name = \"categories\", idx_name=\"category_nk\", source=\"staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f6799",
   "metadata": {},
   "source": [
    "#### Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "58e40271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract staging \n",
    "df_customers = extract_staging(table_name= 'customers', schema_name=DB_SCHEMA_STG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bb669cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform customer \n",
    "customer_tf = transform_customer(data=df_customers, table_name=\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data customers \n",
    "valid_cust, invalid_cust = validation_data(data=customer_tf, table_name=\"customers\",validation_functions={\"email\": validate_email_format,\n",
    "                                                                                                          \"phone\" : validate_phone_format,\n",
    "                                                                                                          \"credit_card_expiration\" : validate_credit_card_expiration_format\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0ecfdd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load valid data from staging to data warehouse\n",
    "load_warehouse(data=valid_cust, schema=\"public\", table_name=\"customers\", idx_name=\"customer_nk\",\n",
    "               source=\"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ce80c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store invalid data to minio\n",
    "if (not invalid_cust.empty) :\n",
    "    handle_error(data=invalid_cust, bucket_name=\"error-dellstore\", table_name=\"customers\", process=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618e590",
   "metadata": {},
   "source": [
    "#### Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0e61ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from staging\n",
    "df_product = extract_staging(table_name='products',schema_name= DB_SCHEMA_STG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0c8d1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data from staging \n",
    "product_tf = transform_product(data=df_product, table_name=\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "40c132d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data products\n",
    "valid_prod, invalid_prod = validation_data(data=product_tf, table_name=\"products\",validation_functions= {\"price\" : validate_price_range})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c6d81fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to data warehouse\n",
    "load_warehouse(data=valid_prod, schema=\"public\", table_name=\"products\", idx_name=\"product_nk\",\n",
    "               source=\"staging\")\n",
    "# store invalid data to minio\n",
    "if (not invalid_prod.empty) :\n",
    "    handle_error(data=invalid_prod, bucket_name=\"error-dellstore\", table_name=\"products\"\n",
    "                 , process=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f225cd",
   "metadata": {},
   "source": [
    "#### Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bfb9f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from staging \n",
    "df_inventory = extract_staging(table_name=\"inventory\", schema_name = DB_SCHEMA_STG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "99f8087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data staging inventory \n",
    "inventory_tf = transform_inventory(data=df_inventory, \n",
    "                                   table_name=\"inventory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "24da0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data after transform to data warehouse\n",
    "load_warehouse(data=inventory_tf, schema=\"public\",table_name=\"inventory\",\n",
    "                idx_name=\"product_nk\", source=\"staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e97de",
   "metadata": {},
   "source": [
    "#### Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0b62adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from staging \n",
    "df_orders = extract_staging(table_name=\"orders\", schema_name=DB_SCHEMA_STG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "29cfe2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data orders\n",
    "orders_tf = transform_orders(data=df_orders, table_name=\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b6fde4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data orders \n",
    "valid_orders, invalid_orders = validation_data(data=orders_tf, table_name=\"orders\",\n",
    "                                               validation_functions={\"net_amount\" : validate_positive_values,\n",
    "                                                                     \"tax\": validate_positive_values,\n",
    "                                                                     \"total_amount\" : validate_positive_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c8dcc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load valid data orders to data warehouse\n",
    "load_warehouse(data=valid_orders, schema=\"public\", table_name=\"orders\",\n",
    "               idx_name=\"order_nk\",\n",
    "               source=\"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2319726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store invalid data to minio\n",
    "if (not invalid_orders.empty) :\n",
    "    handle_error(data=invalid_orders,bucket_name=\"error-dellstore\", \n",
    "                 table_name=\"orders\", process=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fc409",
   "metadata": {},
   "source": [
    "#### Orderlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "99c1c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from staging \n",
    "df_orderlines = extract_staging(table_name=\"orderlines\", schema_name= DB_SCHEMA_STG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f2fa58ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       orderlineid  orderid  prod_id  quantity   orderdate  \\\n",
      "0                1     2001     9702         3  2004-03-04   \n",
      "1                2     2001     3782         3  2004-03-04   \n",
      "2                3     2001     2714         2  2004-03-04   \n",
      "3                4     2001     4178         3  2004-03-04   \n",
      "4                5     2001        4         2  2004-03-04   \n",
      "...            ...      ...      ...       ...         ...   \n",
      "50261            5    12000     7670         2  2004-12-15   \n",
      "50262            6    12000     1054         1  2004-12-15   \n",
      "50263            7    12000     1717         1  2004-12-15   \n",
      "50264            8    12000     1807         3  2004-12-15   \n",
      "50265            9    12000     3576         3  2004-12-15   \n",
      "\n",
      "                      created_at  \n",
      "0     2026-01-19 12:59:03.253602  \n",
      "1     2026-01-19 12:59:03.253602  \n",
      "2     2026-01-19 12:59:03.253602  \n",
      "3     2026-01-19 12:59:03.253602  \n",
      "4     2026-01-19 12:59:03.253602  \n",
      "...                          ...  \n",
      "50261 2026-01-19 12:59:03.253602  \n",
      "50262 2026-01-19 12:59:03.253602  \n",
      "50263 2026-01-19 12:59:03.253602  \n",
      "50264 2026-01-19 12:59:03.253602  \n",
      "50265 2026-01-19 12:59:03.253602  \n",
      "\n",
      "[50266 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_orderlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "26cff513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data orderlines\n",
    "orderlines_tf = transform_orderlines(data=df_orderlines, table_name=\"orderlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "675b2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data orderlines \n",
    "valid_orderlines, invalid_orderlines = validation_data(data=orderlines_tf, table_name=\"orderlines\", \n",
    "                                                       validation_functions={\"quantity\": validate_positive_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "635c62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to data warehouse\n",
    "load_warehouse(data=valid_orderlines, schema=\"public\", table_name=\"orderlines\",\n",
    "               idx_name=['orderline_nk','order_id','product_id', 'quantity'],\n",
    "               source=\"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "de94e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store invalid data to minio\n",
    "if (not invalid_orderlines.empty) : \n",
    "    handle_error(data=invalid_cust, bucket_name=\"error-dellstore\", table_name=\"orderlines\", process=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944a1b0",
   "metadata": {},
   "source": [
    "#### Customer Orders History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from staging table customer_orders_history\n",
    "df_order_hist = extract_staging(table_name=\"customer_orders_history\", schema_name=DB_SCHEMA_STG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8627da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data customser_orders_history\n",
    "cust_order_hist_tf = transform_order_hist_cust(data=df_order_hist, table_name=\"customer_orders_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bda021f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data \n",
    "valid_cust_order_hist, invalid_cust_order_hist = validation_data(data=cust_order_hist_tf, table_name=\"customer_orders_history\", validation_functions={\n",
    "    \"email\" : validate_email_format,\n",
    "    \"phone\" : validate_phone_format,\n",
    "    \"credit_card_expiration\" : validate_credit_card_expiration_format\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f56e7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to warehouse \n",
    "load_warehouse(data=valid_cust_order_hist, \n",
    "               schema=\"public\", \n",
    "               table_name = \"customers\",\n",
    "               idx_name=['customer_nk'],\n",
    "               source=\"staging\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9ec25c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data products \n",
    "prod_order_hist_tf = transform_order_hist_prod(data=df_order_hist, table_name=\"customer_orders_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "01fdb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data products \n",
    "valid_order_hist_prod, invalid_order_hist_prod = validation_data(\n",
    "    data=prod_order_hist_tf,\n",
    "    table_name=\"products\",\n",
    "    validation_functions = {\n",
    "        \"price\" : validate_price_range\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0c45e580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       product_nk                           category_id               title  \\\n",
      "0               3  eca34eae-0461-4be1-a517-6b2237243559  ACADEMY ADAPTATION   \n",
      "1            8910  ba727e59-3780-4c93-8227-7ef9ad761695   ALABAMA TREATMENT   \n",
      "2             155  66c43dc2-a217-43b5-a334-3a25a9a4f268   ACADEMY CLEOPATRA   \n",
      "3            1471  92443c20-6cd0-4816-8ea7-9eee0dab9646          ACE ISLAND   \n",
      "4            4014  92443c20-6cd0-4816-8ea7-9eee0dab9646       AFRICAN ALICE   \n",
      "...           ...                                   ...                 ...   \n",
      "10068        5220  abf303a9-b082-4172-84c5-0f687df7ffd7          AGENT DEER   \n",
      "10070        6166  66c43dc2-a217-43b5-a334-3a25a9a4f268      AIRPLANE COLOR   \n",
      "10076         644  179a94b6-0602-4388-ab22-b547173e273f       ACADEMY OSCAR   \n",
      "10080        8330  92443c20-6cd0-4816-8ea7-9eee0dab9646   ALABAMA FORRESTER   \n",
      "10082        9346  5ee0a3f3-9057-4553-9315-12de10139ad4      ALADDIN GALAXY   \n",
      "\n",
      "               actor  price  special  common_prod_id  \n",
      "0        VIVIEN KAHN  28.99        0            7173  \n",
      "1      REESE DUKAKIS   9.99        0            6727  \n",
      "2      LEELEE BACALL  21.99        0            7745  \n",
      "3          ALAN KAHN   9.99        0            6333  \n",
      "4       MARLON CROWE  26.99        0             724  \n",
      "...              ...    ...      ...             ...  \n",
      "10068   JOHNNY TOMEI  24.99        0            6145  \n",
      "10070  GROUCHO TOMEI  13.99        0            6229  \n",
      "10076   GLENN KEATON  28.99        0            8521  \n",
      "10080    ANNE BENING  24.99        0            3816  \n",
      "10082     JON KUDROW  18.99        0            8201  \n",
      "\n",
      "[6386 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(valid_order_hist_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "cfcf19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_warehouse(data=valid_order_hist_prod, schema=\"public\", table_name=\"products\", idx_name=[\"product_nk\"], source=\"staging\")\n",
    "if (not invalid_order_hist_prod.empty):\n",
    "    handle_error(data=invalid_order_hist_prod, bucket_name='error-dellstore', table_name=\"products\", process='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9ef981b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Orders\n",
    "order_hist_tf = transform_order_hist_order(data=df_order_hist, table_name=\"customer_orders_history\")\n",
    "valid_order_hist, invalid_order_hist = validation_data(data=order_hist_tf, table_name=\"orders\", validation_functions={\"net_amount\": validate_positive_values, \n",
    "                                                                                                           \"tax\": validate_positive_values, \n",
    "                                                                                                           \"total_amount\": validate_positive_values})\n",
    "load_warehouse(data=valid_order_hist, schema=\"public\", table_name=\"orders\", idx_name=[\"order_nk\"], source=\"staging\")\n",
    "if (not invalid_order_hist.empty):\n",
    "    handle_error(data=invalid_order_hist, bucket_name='error-dellstore', table_name=\"orders\", process='validation')\n",
    "\n",
    "# Data Orderlines\n",
    "orderline_hist_tf = transform_order_hist_orderline(data=df_order_hist, table_name=\"customer_orders_history\")\n",
    "valid_orderline_hist, invalid_orderline_hist = validation_data(data=orderline_hist_tf, table_name=\"orderlines\", validation_functions={\"quantity\": validate_positive_values})\n",
    "load_warehouse(data=valid_orderline_hist, schema=\"public\", table_name=\"orderlines\", \n",
    "               idx_name=[\"orderline_nk\",\"order_id\",\"product_id\",\"quantity\"], source=\"staging\")\n",
    "if (not invalid_orderline_hist.empty):\n",
    "    handle_error(data=invalid_orderline_hist, bucket_name='error-dellstore', table_name=\"orderlines\", process='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda392a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8a927c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Customer History\n",
    "df_cust_hist = extract_staging(table_name=\"cust_hist\", schema_name=DB_SCHEMA_STG)\n",
    "cust_hist_tf = transform_cust_hist(data=df_cust_hist, table_name=\"cust_hist\")\n",
    "load_warehouse(data=cust_hist_tf, schema=\"public\", table_name=\"cust_hist\", \n",
    "               idx_name=[\"customer_id\",\"order_id\",\"product_id\"], source=\"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a4b8a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Order Status Analytic\n",
    "df_order_analytic = extract_staging(table_name=\"order_status_analytic\", schema_name=DB_SCHEMA_STG)\n",
    "order_analytic_tf = transform_order_status_analytic(data=df_order_analytic, table_name=\"order_status_analytic\")\n",
    "valid_orders_analytic, invalid_orders_analytic = validation_data(data=order_analytic_tf, table_name=\"order_status_analytic\", validation_functions={\"status\": validate_order_status})\n",
    "load_warehouse(data=valid_orders_analytic, schema=\"public\", table_name=\"order_status_analytic\", \n",
    "               idx_name=\"order_id\", source=\"staging\")\n",
    "if (not invalid_orders_analytic.empty):\n",
    "    handle_error(data=invalid_orders_analytic, bucket_name='error-dellstore', table_name=\"order_status_analytic\", process='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544bd99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
