{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53221393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # for load from env \n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "import re # regex module\n",
    "import logging\n",
    "\n",
    "from minio import Minio \n",
    "from io  import BytesIO\n",
    "\n",
    "from sqlalchemy import create_engine \n",
    "import sqlalchemy \n",
    "from pangres import upsert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83c6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load env variables \n",
    "load_dotenv(\".env\")\n",
    "\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_SCHEMA_STG = os.getenv(\"DB_SCHEMA_STG\")\n",
    "DB_SCHEMA_LOG = os.getenv(\"DB_SCHEMA_LOG\")\n",
    "DB_SCHEMA_DWH = os.getenv(\"DB_SCHEMA_DWH\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\")   \n",
    "\n",
    "# get minio access from env \n",
    "ACCESS_KEY_MINIO = os.getenv(\"ACCESS_KEY_MINIO\")\n",
    "SECRET_KEY_MINIO = os.getenv(\"SECRET_KEY_MINIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d55f4",
   "metadata": {},
   "source": [
    "#### Read SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f230f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql(table_name) :\n",
    "    # open sql file and read content\n",
    "    with open(f\"{MODEL_PATH}{table_name}.sql\",\"r\") as file:\n",
    "        content = file.read()\n",
    "    # return to text query\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152d7dd",
   "metadata": {},
   "source": [
    "#### ETL Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a33ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_log(log_msg: dict) :\n",
    "    try :\n",
    "        #create connection database \n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "\n",
    "        # change dictionary log_msg to dataframe\n",
    "        df_log = pd.DataFrame([log_msg])\n",
    "\n",
    "        #extract data log \n",
    "        with conn.connect() as connection :\n",
    "            df_log.to_sql(\n",
    "                name = \"etl_log\",\n",
    "                con = connection,\n",
    "                schema = \"log\",\n",
    "                if_exists = \"append\",\n",
    "                index = False\n",
    "            )\n",
    "    except Exception as e :\n",
    "        logging.error(f\"Cant save your log message. Error: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735afb71",
   "metadata": {},
   "source": [
    "#### Read ETL Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fccfd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_etl_log(filter_params: dict) :\n",
    "    try :\n",
    "        # create connection to database \n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "\n",
    "        # get etl_date from latest\n",
    "        query = sqlalchemy.text(read_sql(\"log\"))\n",
    "\n",
    "        # execute query with pd.read_sql \n",
    "        with conn.connect() as connection :\n",
    "            df = pd.read_sql(\n",
    "                sql = query,\n",
    "                con = connection,\n",
    "                params = (filter_params,)\n",
    "            )\n",
    "        # return extracted data \n",
    "        return df\n",
    "    except Exception as  e :\n",
    "        logging.error(f\"Cant execute your query. Error: {e}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a77a0",
   "metadata": {},
   "source": [
    "#### Extract Staging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88bc7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_staging(table_name:str, schema_name:str) -> pd.DataFrame :\n",
    "    try :\n",
    "        # create connection to database \n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "\n",
    "        # get date from previous process in etl_log \n",
    "        filter_log = {\n",
    "            \"step_name\" : \"warehouse\",\n",
    "            \"table_name\" : table_name,\n",
    "            \"status\" : \"success\",\n",
    "            \"process\" : \"load\"\n",
    "        }\n",
    "        etl_date = read_etl_log(filter_log) \n",
    "\n",
    "        # if previous process is null, set etl_date to 1990-01-01\n",
    "        # if previous process is not null, get the latest etl_date \n",
    "        if (etl_date['max'][0] == None) :\n",
    "            etl_date = '1990-01-01'\n",
    "        else :\n",
    "            etl_date = etl_date[max][0]\n",
    "\n",
    "        # create query to select all column from specified table where created_at > etl_date\n",
    "        query = f\"SELECT * FROM {schema_name}.{table_name} WHERE created_at > %s::timestamp\"\n",
    "\n",
    "        # execute the query with pd.read_sql \n",
    "        with conn.connect() as connection :\n",
    "            df = pd.read_sql(\n",
    "                sql=query,\n",
    "                con=connection,\n",
    "                params=(etl_date,)\n",
    "            )\n",
    "        log_msg = {\n",
    "            \"step\"  : \"warehouse\",\n",
    "            \"process\" : \"extraction\",\n",
    "            \"status\" : \"success\",\n",
    "            \"source\" :  \"database\",\n",
    "            \"table_name\" : table_name, \n",
    "            \"etl_date\" : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        return df\n",
    "        \n",
    "    except Exception as  e :\n",
    "        log_msg = {\n",
    "            \"step\"  : \"warehouse\",\n",
    "            \"process\" : \"extraction\",\n",
    "            \"status\" : \"failed\",\n",
    "            \"source\" :  \"database\",\n",
    "            \"table_name\" : table_name, \n",
    "            \"etl_date\" : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        logging.error(f\"Cant execute your query. Error: {e}\", exc_info=True)\n",
    "        raise\n",
    "    finally :\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e47d2",
   "metadata": {},
   "source": [
    "#### Handle Error MINIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a710276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to handle error data and upload it to minio\n",
    "def handle_error(data, bucket_name: str, table_name: str, process: str) :\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    try : \n",
    "        client = Minio('localhost:9000',\n",
    "                       access_key=ACCESS_KEY_MINIO,\n",
    "                       secret_key=SECRET_KEY_MINIO,\n",
    "                       secure=False)\n",
    "        # check if bucket exists, if not exists then create it\n",
    "        if not client.bucket_exists(bucket_name):\n",
    "            client.make_bucket(bucket_name)\n",
    "        \n",
    "        # convert dataframe to csv and then to bytes\n",
    "        csv_bytes = data.to_csv().encode('utf-8')\n",
    "        csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "        # upload the csv file to the bucket \n",
    "        client.put_object(\n",
    "            bucket_name=bucket_name,\n",
    "            object_name=f\"{process}_{table_name}_{current_date}.csv\",\n",
    "            data=csv_buffer,\n",
    "            length=len(csv_bytes),\n",
    "            content_type='application/csv'\n",
    "        )\n",
    "\n",
    "        # list objects in the bucket \n",
    "        objects = client.list_objects(bucket_name, recursive=True)\n",
    "        for obj in objects:\n",
    "            logging.info(f\"Object in bucket: {obj.object_name}\")\n",
    "    except Exception as e :\n",
    "        logging.error(f\"Failed to upload error data to Minio. Error: {e}\", exc_info=True)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d1f4e",
   "metadata": {},
   "source": [
    "#### Load Warehouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d45658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for load data to data warehouse\n",
    "def load_warehouse(data, schema:str, table_name:str, idx_name:str, source):\n",
    "    try :\n",
    "        # create connection to database \n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "\n",
    "        # set data index or primary key\n",
    "        data = data.set_index(idx_name)\n",
    "\n",
    "        # do upsert ( insert for non existing data, update for existing data )\n",
    "        with conn.connect() as connection :\n",
    "            upsert(\n",
    "                df = data,\n",
    "                con = connection,\n",
    "                table_name = table_name,\n",
    "                schema = schema,\n",
    "                if_row_exists = \"update\"\n",
    "            )\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\":\"load\",\n",
    "            \"status\": \"success\",\n",
    "            \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\":\"load\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "\n",
    "        }\n",
    "        handle_error(data, bucket_name=\"error-dellstore\", table_name=table_name, process=\"load\")\n",
    "    finally :\n",
    "        etl_log(log_msg)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ae41e",
   "metadata": {},
   "source": [
    "#### Extract Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c105467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to get data from data warehouse to obtain value of foreign key\n",
    "def extract_target(table_name:str) :\n",
    "    conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
    "    query = f\"SELECT * FROM public.{table_name}\"\n",
    "    with conn.connect() as connection :\n",
    "        df = pd.read_sql(\n",
    "            sql = query,\n",
    "            con = connection    \n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97688122",
   "metadata": {},
   "source": [
    "#### Transform Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e77062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table category from staging to data warehouse\n",
    "## - rename column from category to category_nk\n",
    "## - rename column from categoryname to category_name\n",
    "\n",
    "def transform_categories(data: pd.DataFrame, table_name:str) -> pd.DataFrame :\n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "        data = data.rename(column={'category' : 'category_nk',\n",
    "                                   'categoryname' : 'category_name'})\n",
    "        \n",
    "        # remove dupicate based on category_nk and category_name \n",
    "        data = data.drop_duplicates(subset='category_nk')\n",
    "\n",
    "        # drop column created_at \n",
    "        data = data.drop(columns=['created_at'])\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"success\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        handle_error(data = data, \n",
    "                     bucket_name = 'error_dellstore',\n",
    "                     table_name = table_name, \n",
    "                     process = process)\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae259777",
   "metadata": {},
   "source": [
    "#### Transform Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7273687",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table customer from staging to data warehouse\n",
    "## - rename column from creditcardtype to credit_card_type\n",
    "## - rename column from creditcard to credit_card\n",
    "## - rename column from creditcardexplanation to credit_card_explanation\n",
    "## - rename column from firstname to first_name \n",
    "## - rename column from lastname to last_name\n",
    "\n",
    "def transform_customer(data: pd.DataFrame, table_name:str) -> pd.DataFrame :\n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "        data = data.rename(columns={'customerid': 'customer_nk',\n",
    "                            'firstname': 'first_name',\n",
    "                            'lastname': 'last_name',\n",
    "                            'creditcardtype': 'credit_card_type',\n",
    "                            'creditcard': 'credit_card',\n",
    "                            'creditcardexpiration': 'credit_card_expiration'})\n",
    "\n",
    "        # remove dupicate based on customer_nk and customer_name \n",
    "        data = data.drop_duplicates(subset='customer_nk')\n",
    "\n",
    "        # masking credit_card_number \n",
    "        data['credit_card'] = data['credit_card'].apply(lambda x: re.sub(r'\\d', 'X', x[:-4]) + x[-4:])\n",
    "\n",
    "        # drop column created_at \n",
    "        data = data.drop(columns=['created_at'])\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"success\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        handle_error(data = data, \n",
    "                     bucket_name = 'error_dellstore',\n",
    "                     table_name = table_name, \n",
    "                     process = process)\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b56a4",
   "metadata": {},
   "source": [
    "#### Transform product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0a05bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table products from staging to data warehouse\n",
    "## - rename column from prod_id to product_nk\n",
    "## - rename column from category to category_nk\n",
    "## - lookup category_nk from categories tables based on category\n",
    "def transform_product(data: pd.DataFrame, table_name:str) -> pd.DataFrame :\n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "\n",
    "        #rename column product \n",
    "        data = data.rename(columns= {\n",
    "            \"prod_id\" : \"product_nk\",\n",
    "            \"category\" : \"category_nk\"\n",
    "        })\n",
    "\n",
    "        # remove duplicate based on product_nk \n",
    "        data = data.drop_duplicates(subset='product_nk')\n",
    "\n",
    "        # extract data from the 'categories' table \n",
    "        categories = extract_target('categories')\n",
    "\n",
    "        # lookup 'category_id' from categories table based on 'category'\n",
    "        data['category_id'] = data['category_nk'].apply(lambda x: categories.loc[categories['category_nk']== x, 'category_id'].values[0])\n",
    "        # category_mapping = categories.set_index('category_nk')['category_id']\n",
    "        # data['category_id'] = data['category_nk'].map(category_mapping)\n",
    "\n",
    "        # drop column created_at\n",
    "        data = data.drop(columns=['created_at','category_nk'])\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\" : process, \n",
    "            \"status\" : \"success\",\n",
    "            \"source\" : \"staging\",\n",
    "            \"table_name\"  : table_name, \n",
    "            \"etl_date\" : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "\n",
    "        return data \n",
    "    \n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\" : process, \n",
    "            \"status\" : \"failed\",\n",
    "            \"source\" : \"staging\",\n",
    "            \"table_name\"  : table_name, \n",
    "            \"etl_date\" : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        handle_error(data = data, \n",
    "                     bucket_name = 'error_dellstore',\n",
    "                     table_name = table_name, \n",
    "                     process = process)    \n",
    "    finally :\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f35336",
   "metadata": {},
   "source": [
    "#### Transform Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25a7a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table inventory from staging to data warehouse\n",
    "## - rename column quan_in_stock to quantity_in_stock\n",
    "## - column product_id refers to products table using column product_nk\n",
    "\n",
    "def transform_inventory(data: pd.DataFrame, table_name: str) -> pd.DataFrame : \n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "\n",
    "        # rename column inventory\n",
    "        data = data.rename(columns={\n",
    "            \"quan_in_stock\" : \"quantity_in_stock\",\n",
    "            \"prod_id\" : \"product_nk\"\n",
    "        }) \n",
    "\n",
    "        # remove duplicate based on product_nk \n",
    "        data = data.drop_duplicates(subset='product_nk')\n",
    "\n",
    "        #extract data from the 'products' table \n",
    "        products = extract_target('products')\n",
    "\n",
    "        # lookup 'product_id' from products table based on 'product_nk'\n",
    "        data['product_id'] = data['product_nk'].apply(lambda x : products.loc[products['product_nk']== x, 'product_id'].values[0])\n",
    "        # coba pake map \n",
    "        # products_mapping = products.set_index('product_nk')['product_id']\n",
    "        # data['product_id'] = data['product_nk'].map(products_mapping)\n",
    "\n",
    "        # drop column created_at \n",
    "        data = data.drop(columns=['created_at'])\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"success\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "        return data\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\" : str(e)\n",
    "        }\n",
    "        handle_error(data = data, \n",
    "                     bucket_name = 'error_dellstore',\n",
    "                     table_name = table_name, \n",
    "                     process = process)\n",
    "    finally :\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ea492",
   "metadata": {},
   "source": [
    "#### Transform Orders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b8dc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transformation table inventory from staging to data warehouse\n",
    "## - rename column orderid to order_nk \n",
    "## - get customer_id values from customer table by matching customer_nk \n",
    "## - rename orderdate to order_date \n",
    "## - rename netamount to net_amount \n",
    "## - rename totalamount to total_amount \n",
    "\n",
    "def transform_orders(data: pd.DataFrame, table_name: str) -> pd.DataFrame : \n",
    "    try :\n",
    "        \n",
    "        process = \"transformation\"\n",
    "\n",
    "        # rename column \n",
    "        data = data.rename(columns = {\n",
    "            \"orderid\" : \"order_nk\",\n",
    "            \"orderdate\" : \"order_date\",\n",
    "            \"netamount\" : \"net_amount\",\n",
    "            \"totalamount\" : \"total_amount\",\n",
    "            \"customerid\" : \"customer_nk\"\n",
    "        })\n",
    "\n",
    "        # extract table customer \n",
    "        customers = extract_target(\"customers\")\n",
    "\n",
    "        # lookup customer_id from customer table based on customer_nk\n",
    "        data['customer_id'] = data['customer_nk'].apply(lambda x : \n",
    "                                                        customers.loc[customers['customer_nk']==x, 'customer_id'].values[0])\n",
    "        # or using map\n",
    "        # customer_mapping = customers.set_index('customer_nk')['customer_id']  \n",
    "        # data['customer_id'] = data['customer_nk'].map(customer_mapping)\n",
    "\n",
    "        # drop column created at and customer_nk (for identifier customer can use customer_id column)\n",
    "        data = data.drop(columns = ['created_at', 'customer_nk'])\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        return data\n",
    "\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\" : str(e)\n",
    "                }\n",
    "        handle_error(\n",
    "            data=data,\n",
    "            bucket_name='error-dellstore',\n",
    "            table_name = table_name,\n",
    "            process=process\n",
    "        )\n",
    "\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824bbd0",
   "metadata": {},
   "source": [
    "#### Transform Orderline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "334177bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for transform orders table from staging to data warehouse \n",
    "## - rename column orderlineid to orderline_nk\n",
    "## - rename column orderid to order_id and lookup from orders table based on orderid\n",
    "## - rename column prod_id to product_id and lookup from products based on prod_id\n",
    "## - rename column orderdate to order_date \n",
    "\n",
    "def transform_orderlines(data: pd.DataFrame, table_name: str) -> pd.DataFrame :\n",
    "    try :\n",
    "        process = \"transformation\"\n",
    "\n",
    "        #rename column \n",
    "        data = data.rename(columns= {\n",
    "            \"orderlineid\" : \"orderline_nk\",\n",
    "            \"order_id\" : \"order_nk\",\n",
    "            \"prod_id\" : \"product_nk\",\n",
    "            \"orderdate\"  :\"order_date\"\n",
    "        })\n",
    "\n",
    "        # extract data from orders table \n",
    "        orders = extract_target(\"orders\")\n",
    "\n",
    "        #lookup 'order_id' \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        #or using map \n",
    "        # orders_mapping = orders.set_index('order_nk')['order_id']\n",
    "        # data['order_id'] = data['order_nk'].map(orders_mapping)\n",
    "\n",
    "        # extract data from products table \n",
    "        products = extract_target(\"products\")\n",
    "\n",
    "        #lookup product id \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: orders.loc[orders['product_nk'] == x, 'order_id'].values[0])\n",
    "        #or using map \n",
    "        # products_mapping = products.set_index('product_nk')['product_id']\n",
    "        # data['product_id'] = data['product_nk'].map(products_mapping)\n",
    "\n",
    "        # drop unnecessary columns \n",
    "        data = data.drop(columns = ['created_at', 'order_nk', 'product_nk'])\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except Exception as e :\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        handle_error(\n",
    "            data= data,\n",
    "            bucket_name= 'error_dellstore',\n",
    "            table_name = table_name,\n",
    "            process= process\n",
    "        )\n",
    "\n",
    "    finally :\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b482f3",
   "metadata": {},
   "source": [
    "#### Transform customer_orders_history\n",
    "\n",
    "Target:\n",
    "- Table customers\n",
    "- Table product\n",
    "- Table orders\n",
    "- Table orderlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67b7e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_cust(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column for customers\n",
    "        data = data.rename(columns={\n",
    "                                'customer_id': 'customer_nk',\n",
    "                                'customer_firstname': 'first_name',\n",
    "                                'customer_lastname': 'last_name',\n",
    "                                'customer_address1': 'address1',\n",
    "                                'customer_address2': 'address2',\n",
    "                                'customer_city': 'city',\n",
    "                                'customer_state': 'state',\n",
    "                                'customer_zip': 'zip',\n",
    "                                'customer_country': 'country',\n",
    "                                'customer_region': 'region',\n",
    "                                'customer_email': 'email',\n",
    "                                'customer_phone': 'phone',\n",
    "                                'customer_creditcardtype': 'credit_card_type',\n",
    "                                'customer_creditcard': 'credit_card',\n",
    "                                'customer_creditcardexpiration': 'credit_card_expiration',\n",
    "                                'customer_username': 'username',\n",
    "                                'customer_password': 'password',\n",
    "                                'customer_age': 'age',\n",
    "                                'customer_income': 'income',\n",
    "                                'customer_gender': 'gender'\n",
    "                            }) \n",
    "        \n",
    "        columns_to_keep = [\n",
    "            'customer_nk', 'customer_id', 'first_name', 'last_name', \n",
    "            'address1', 'address2', 'city', 'state', 'zip', \n",
    "            'country', 'region', 'email', 'phone', \n",
    "            'credit_card_type', 'credit_card', 'credit_card_expiration', \n",
    "            'username', 'password', 'age', 'income', 'gender'\n",
    "        ]\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=[col for col in data.columns if col not in columns_to_keep])\n",
    "\n",
    "        # Deduplication based on customer_nk\n",
    "        data = data.drop_duplicates(subset='customer_nk')\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55711a",
   "metadata": {},
   "source": [
    "#### Transform Customer History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc80cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cust_hist(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column orderlines\n",
    "        data = data.rename(columns={'customerid':'customer_nk', \n",
    "                                    'prod_id':'product_nk', \n",
    "                                    'orderid':'order_nk'})\n",
    "        \n",
    "        # Extract data from the `customers` table\n",
    "        customers = extract_target('customers')\n",
    "\n",
    "        # Lookup `customer_id` from `customers` table based on `customerid`   \n",
    "        data['customer_id'] = data['customer_nk'].apply(lambda x: customers.loc[customers['customer_nk'] == x, 'customer_id'].values[0])\n",
    "        \n",
    "\n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # Extract data from the `product` table\n",
    "        products = extract_target('products')\n",
    "\n",
    "        # Lookup `product_id` from `product` table based on `prod_id`   \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: products.loc[products['product_nk'] == x, 'product_id'].values[0])\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        data = data.drop(columns=['customer_nk','order_nk','product_nk'])\n",
    "\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29064e4",
   "metadata": {},
   "source": [
    "#### Transform order status analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebc80a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_status_analytic(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column order_status_analytic\n",
    "        data = data.rename(columns={'orderid':'order_nk'})\n",
    "\n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        data = data.drop(columns='created_at')\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
