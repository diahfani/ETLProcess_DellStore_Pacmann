{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week #4 - Live Class\n",
    "Data Pipeline Course - Sekolah Engineer - Pacmann Academy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation method:\n",
    "\n",
    "- Enrichment: Adding additional information or attributes to the data.\n",
    "- Aggregation: Combining multiple data points into a single summary.\n",
    "- Joining: Combining data from multiple sources based on a common key.\n",
    "- Anonymization: Removing or obfuscating personally identifiable information from the data.\n",
    "- Filtering: Selecting specific data points based on certain criteria.\n",
    "- Splitting: Dividing a single data point into multiple parts.\n",
    "- Structuring: Organizing the data into a specific format or structure.\n",
    "- Deduplication: Removing duplicate data points from the dataset.\n",
    "- Conversion: Changing the data format or type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "\n",
    "Data validation is the process of ensuring that data is accurate, complete, and consistent. It may involve checking:\n",
    "- missing values, \n",
    "- verifying data types, \n",
    "- performing range checks, \n",
    "- or applying any other rules or constraints to ensure the quality and integrity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Case: Dell DVD Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Description\n",
    "\n",
    "`Problem`\n",
    "\n",
    "The Dell DVD Store is facing challenges with its current data processing. The store needs to handle data from multiple sources such as spreadsheets, databases, and APIs. The key challenges include:\n",
    "- Database: The Dell DVD Store saves data from current system.\n",
    "- API: Retrieves data from the old system and contains historical data from the old system.\n",
    "- Spreadsheet: Contains analysis results from the team about order status based on the current product stock.\n",
    "\n",
    "<img src= https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/live_w4_2.png width=\"1000\"> <br>\n",
    "\n",
    "`Solution`\n",
    "\n",
    "To address these challenges, we propose creating a comprehensive data pipeline for the Dell DVD Store. This pipeline will involve the following steps:\n",
    "- Data Extraction:\n",
    "Sources: Extract data from spreadsheets, databases, and APIs.\n",
    "Techniques: Use both full and incremental extraction methods to retrieve data efficiently.\n",
    "- Data Load:\n",
    "Staging: Load raw data into a staging database (PostgreSQL) without transformation.\n",
    "Final Load: Transfer clean and transformed data to the final destination.\n",
    "Failure Handling: Log failed data loads to MinIO object storage for reprocessing\n",
    "- Data Transformation:\n",
    "Cleaning: Handle missing values, incorrect data formats, and other data quality issues.\n",
    "Trasnforming: Add derived fields and calculated metrics as needed.\n",
    "- Data Validation: process of checking and ensuring that data meets predefined rules\n",
    "\n",
    "`Tools and Technologies`:\n",
    "- Python: For build Data Pipeline\n",
    "- PostgreSQL: For log, staging and final data storage.\n",
    "- MinIO: For load failed data and invalid data.\n",
    "- Docker: For running MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Live Class\n",
    "- Performe Data Ingestion (Extract and Load) from Source to Staging Area\n",
    "- Load Failed Data to Object Storage\n",
    "\n",
    "### Next Task\n",
    "- Extract Data From Staging\n",
    "- Transformation Data\n",
    "- Validation Data\n",
    "- Load Invalid Data to Object Storage\n",
    "- Load Valid Data to Data Warehouse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src= https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/live_w4_1.png width=\"800\"> <br>\n",
    "DDL Schema: [Link](https://drive.google.com/file/d/1NUNR84AGnHDbxrsEhIXWnJZ-MmPTlrKa/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source to Target Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source to Target Mapping Documentation: [Link](https://github.com/Kurikulum-Sekolah-Pacmann/data_pipeline_dellstore/blob/main/source_target_mapping.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some validation rules that can be applied to the previously mentioned tables to ensure data integrity and quality.\n",
    "\n",
    "Validation Rule:\n",
    "1. Customer Table Validation:\n",
    "    - Validate that email addresses conform to standard formats (e.g., yahoo.com, hotmail.com, gmail.com).\n",
    "    - Ensure that the phone number contains exactly 10 digits.\n",
    "    - Validate the credit card expiration date format to be in the YYYY/MM format.\n",
    "2. Products Table Validation\n",
    "    - Ensure that the price value is within the range of 0 to 100.\n",
    "3. Orders Table Validation:\n",
    "    - Ensure that net_amount, tax, and total_amount are positive values.\n",
    "4. Orderline Table Validation\n",
    "    - Ensure that quantity is a positive number.\n",
    "5. order_status_analytic Table Validation\n",
    "    - Validate that the status is either partial, fulfilled, or backordered.\n",
    "\n",
    "Other constraints, such as NOT NULL columns, are managed by the database constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging to Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "from minio import Minio\n",
    "from io import BytesIO\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "from pangres import upsert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load File Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Get the database environment variables\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "\n",
    "DB_NAME_STG = os.getenv(\"DB_NAME_STG\")\n",
    "DB_SHCHEMA_STG = os.getenv(\"DB_SHCHEMA_STG\")\n",
    "DB_NAME_log = os.getenv(\"DB_NAME_log\")\n",
    "DB_NAME_DW = os.getenv(\"DB_NAME_DW\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "# Get the Minio environment variables\n",
    "ACCESS_KEY_MINIO = os.getenv(\"ACCESS_KEY_MINIO\")\n",
    "SECRET_KEY_MINIO = os.getenv(\"SECRET_KEY_MINIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage SQL queries efficiently using external `.sql` files, you can create a function that reads these files and returns their content.<br>\n",
    "Each `.sql` file should contain the SQL query for the respective table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql(table_name):\n",
    "    #open your file .sql\n",
    "    with open(f\"{MODEL_PATH}{table_name}.sql\", 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    #return query text\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_log(log_msg: dict):\n",
    "    \"\"\"\n",
    "    This function is used to save the log message to the database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_log}\")\n",
    "        \n",
    "        # convert dictionary to dataframe\n",
    "        df_log = pd.DataFrame([log_msg])\n",
    "\n",
    "        #extract data log\n",
    "        df_log.to_sql(name = \"etl_log\",  # Your log table\n",
    "                        con = conn,\n",
    "                        if_exists = \"append\",\n",
    "                        index = False)\n",
    "    except Exception as e:\n",
    "        print(\"Can't save your log message. Cause: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_etl_log(filter_params: dict):\n",
    "    \"\"\"\n",
    "    This function read_etl_log that reads log information from the etl_log table and extracts the maximum etl_date for a specific process, step, table name, and status.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_log}\")\n",
    "        \n",
    "        # To help with the incremental process, get the etl_date from the relevant process\n",
    "        \"\"\"\n",
    "        SELECT MAX(etl_date)\n",
    "        FROM etl_log \"\n",
    "        WHERE \n",
    "            step = %s and\n",
    "            table_name ilike %s and\n",
    "            status = %s and\n",
    "            process = %s\n",
    "        \"\"\"\n",
    "        query = sqlalchemy.text(read_sql(\"log\"))\n",
    "\n",
    "        # Execute the query with pd.read_sql\n",
    "        df = pd.read_sql(sql=query, con=conn, params=(filter_params,))\n",
    "\n",
    "        #return extracted data\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Can't execute your query. Cause: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data From Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_staging(table_name: str, schema_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to extract data from the staging database. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create connection to database staging\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_STG}\")\n",
    "\n",
    "        # Get date from previous process\n",
    "        filter_log = {\"step_name\": \"warehouse\",\n",
    "                    \"table_name\": table_name,\n",
    "                    \"status\": \"success\",\n",
    "                    \"process\": \"load\"}\n",
    "        etl_date = read_etl_log(filter_log)\n",
    "\n",
    "\n",
    "        # If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "        # Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "        if(etl_date['max'][0] == None):\n",
    "            etl_date = '1111-01-01'\n",
    "        else:\n",
    "            etl_date = etl_date[max][0]\n",
    "            # etl_date = etl_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "        query = f\"SELECT * FROM {schema_name}.{table_name} WHERE created_at > %s::timestamp\"\n",
    "\n",
    "        # Execute the query with pd.read_sql\n",
    "        df = pd.read_sql(sql=query, con=conn, params=(etl_date,))\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"database\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\":\"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Error Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error(data, bucket_name:str, table_name:str, process:str):\n",
    "    \"\"\"\n",
    "    This function is used to handle error or invalid data by uploading the DataFrame to a MinIO bucket.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Initialize MinIO client\n",
    "    client = Minio('localhost:9000',\n",
    "                access_key=ACCESS_KEY_MINIO,\n",
    "                secret_key=SECRET_KEY_MINIO,\n",
    "                secure=False)\n",
    "\n",
    "    # Make a bucket if it doesn't exist\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "\n",
    "    # Convert DataFrame to CSV and then to bytes\n",
    "    csv_bytes = data.to_csv().encode('utf-8')\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    # Upload the CSV file to the bucket\n",
    "    client.put_object(\n",
    "        bucket_name=bucket_name,\n",
    "        object_name=f\"{process}_{table_name}_{current_date}.csv\", #name the fail source name and current etl date\n",
    "        data=csv_buffer,\n",
    "        length=len(csv_bytes),\n",
    "        content_type='application/csv'\n",
    "    )\n",
    "\n",
    "    # List objects in the bucket\n",
    "    objects = client.list_objects(bucket_name, recursive=True)\n",
    "    for obj in objects:\n",
    "        print(obj.object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load to Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_warehouse(data, schema:str, table_name: str, idx_name:str, source):\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_DW}\")\n",
    "        \n",
    "        # set data index or primary key\n",
    "        data = data.set_index(idx_name)\n",
    "        \n",
    "        # Do upsert (Update for existing data and Insert for new data)\n",
    "        upsert(con = conn,\n",
    "                df = data,\n",
    "                table_name = table_name,\n",
    "                schema = schema,\n",
    "                if_row_exists = \"update\")\n",
    "        \n",
    "        #create success log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\":\"load\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": source,\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        # return data\n",
    "    except Exception as e:\n",
    "\n",
    "        #create fail log message\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\":\"load\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") , # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process='load')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        etl_log(log_msg)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Function to extract data from data warehouse to obtain value of foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target(table_name: str):\n",
    "    \"\"\"\n",
    "    this function is used to extract data from the data warehouse.\n",
    "    \"\"\"\n",
    "    conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_DW}\")\n",
    "\n",
    "    # Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "    # Execute the query with pd.read_sql\n",
    "    df = pd.read_sql(sql=query, con=conn)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: categories\n",
    "- Target Table: categories\n",
    "\n",
    "| Source Field | Target Field  | Transformation Rule                                  |\n",
    "|--------------|---------------|------------------------------------------------------|\n",
    "| category     | category_nk   | Direct Mapping                                       |\n",
    "| -            | category_id   | Auto Generated using `uuid_generate_v4()`            |\n",
    "| categoryname | category_name | Direct Mapping                                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categories(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data categoriy from staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column category to category_nk\n",
    "        data = data.rename(columns={'category':'category_nk', 'categoryname':'category_name'})\n",
    "\n",
    "        # deduplication based on category_nk and category name\n",
    "        data = data.drop_duplicates(subset='category_nk')\n",
    "\n",
    "        # drop column created_at\n",
    "        data = data.drop(columns=['created_at'])\n",
    "        \n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": \"category\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": \"category\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        \n",
    "         # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: customers\n",
    "- Target Table: customers\n",
    "\n",
    "| Source Field         | Target Field           | Transformation Rule                                                  |\n",
    "|----------------------|------------------------|----------------------------------------------------------------------|\n",
    "| customerid           | customer_nk            | Direct Mapping                                                       |\n",
    "| -                    | customer_id            | Auto Generated using `uuid_generate_v4()`                            |\n",
    "| firstname            | first_name             | Direct Mapping                                                       |\n",
    "| lastname             | last_name              | Direct Mapping                                                       |\n",
    "| address1             | address1               | Direct Mapping                                                       |\n",
    "| address2             | address2               | Direct Mapping                                                       |\n",
    "| city                 | city                   | Direct Mapping                                                       |\n",
    "| state                | state                  | Direct Mapping                                                       |\n",
    "| zip                  | zip                    | Direct Mapping                                                       |\n",
    "| country              | country                | Direct Mapping                                                       |\n",
    "| region               | region                 | Direct Mapping                                                       |\n",
    "| email                | email                  | Direct Mapping                                                       |\n",
    "| phone                | phone                  | Direct Mapping                                                       |\n",
    "| creditcardtype       | credit_card_type       | Direct Mapping                                                       |\n",
    "| creditcard           | credit_card            | Direct Mapping  and Masking Value                                    |\n",
    "| creditcardexpiration | credit_card_expiration | Direct Mapping                                                       |\n",
    "| username             | username               | Direct Mapping                                                       |\n",
    "| password             | password               | Direct Mapping                                                       |\n",
    "| age                  | age                    | Direct Mapping                                                       |\n",
    "| income               | income                 | Direct Mapping                                                       |\n",
    "| gender               | gender                 | Direct Mapping                                                       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_customer(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data customer from staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column customer\n",
    "        data = data.rename(columns={'customerid':'customer_nk', 'firstname':'first_name', \n",
    "                                    'lastname':'last_name', 'address':'address', 'city':'city', 'state':'state',\n",
    "                                    'zip':'zip', 'email':'email', 'creditcardtype':'credit_card_type', \n",
    "                                    'creditcard':'credit_card', 'creditcardexpiration':'credit_card_expiration', \n",
    "                                    'username':'username', 'password':'password'})\n",
    "        \n",
    "        # deduplication based on customer_nk\n",
    "        data = data.drop_duplicates(subset='customer_nk')\n",
    "\n",
    "        # Masking credit card number\n",
    "        data['credit_card'] = data['credit_card'].apply(lambda x: re.sub(r'\\d', 'X', x[:-4]) + x[-4:])\n",
    "\n",
    "        # drop column created_at\n",
    "        data = data.drop(columns=['created_at'])\n",
    "        \n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        \n",
    "         # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: products\n",
    "- Target Table: products\n",
    "\n",
    "| Source Field  | Target Field     | Transformation Rule                                                                 |\n",
    "|---------------|------------------|-------------------------------------------------------------------------------------|\n",
    "| prod_id       | product_nk       | Direct Mapping                                                                      |\n",
    "| -             | product_id       | Auto Generated using `uuid_generate_v4()`                                           |\n",
    "| category      | category_id      | Lookup `category_id` from `categories` table based on `category`                    |\n",
    "| title         | title            | Direct Mapping                                                                      |\n",
    "| actor         | actor            | Direct Mapping                                                                      |\n",
    "| price         | price            | Direct Mapping                                                                      |\n",
    "| special       | special          | Direct Mapping                                                                      |\n",
    "| common_prod_id| common_prod_id   | Direct Mapping                                                                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_product(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data product from staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column product\n",
    "        data = data.rename(columns={'prod_id':'product_nk', 'category':'category_nk'})\n",
    "        \n",
    "        # deduplication based on product_nk\n",
    "        data = data.drop_duplicates(subset='product_nk')\n",
    "\n",
    "        # Extract data from the `categories` table\n",
    "        categories = extract_target('categories')\n",
    "\n",
    "        #Lookup `category_id` from `categories` table based on `category`   \n",
    "        data['category_id'] = data['category_nk'].apply(lambda x: categories.loc[categories['category_nk'] == x, 'category_id'].values[0])\n",
    "        \n",
    "        # drop column created_at\n",
    "        data = data.drop(columns=['created_at','category_nk'])\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        \n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: inventory\n",
    "- Target Table: inventory\n",
    "\n",
    "| Source Field  | Target Field     | Transformation Rule                                  |\n",
    "|---------------|------------------|------------------------------------------------------|\n",
    "| prod_id       | product_nk       | Direct Mapping                                       |\n",
    "| -             | product_id       | Use the product_id from the product table by matching the product_nk (source)          |\n",
    "| quan_in_stock | quantity_stock   | Direct Mapping                                       |\n",
    "| sales         | sales            | Direct Mapping                                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inventory(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data inventory from staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column inventory\n",
    "        data = data.rename(columns={'prod_id':'product_nk', 'quan_in_stock':'quantity_stock'})\n",
    "        \n",
    "        # deduplication based on product_nk\n",
    "        data = data.drop_duplicates(subset='product_nk')\n",
    "\n",
    "        # Extract data from the `categories` table\n",
    "        products = extract_target('products')\n",
    "\n",
    "        #Lookup `category_id` from `categories` table based on `category`   \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: products.loc[products['product_nk'] == x, 'product_id'].values[0])\n",
    "        \n",
    "        # drop column created_at\n",
    "        data = data.drop(columns=['created_at'])\n",
    "\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        \n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: orders\n",
    "- Target Table: orders\n",
    "\n",
    "| Source Field | Target Field  | Transformation Rule                                                                 |\n",
    "|--------------|---------------|-------------------------------------------------------------------------------------|\n",
    "| -            | order_id      | Auto Generated using `uuid_generate_v4()`                                           |\n",
    "| orderid     | order_nk      | Direct Mapping                                                                      |\n",
    "| customerid  | customer_id   | Use the customer_id from the customer table by matching the customer_nk (source)    |\n",
    "| orderdate   | order_date    | Direct Mapping                                                                      |\n",
    "| status       | status        | Direct Mapping                                                                      |\n",
    "| netammount   | net_ammount    | Direct Mapping                                                                      |\n",
    "| tax       | tax        | Direct Mapping                                                                      |\n",
    "| totalammount   | total_ammount    | Direct Mapping                                                                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_orders(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data orders from staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column orders\n",
    "        data = data.rename(columns={'orderid':'order_nk', 'customerid':'customer_nk', 'orderdate':'order_date', \n",
    "                                    'netamount':'net_amount', 'tax':'tax', 'totalamount':'total_amount'})\n",
    "        \n",
    "        # Extract data from the `customer` table\n",
    "        customers = extract_target('customers')\n",
    "\n",
    "        # Lookup `customer_id` from `customer` table based on `customer_nk`   \n",
    "        data['customer_id'] = data['customer_nk'].apply(lambda x: customers.loc[customers['customer_nk'] == x, 'customer_id'].values[0])\n",
    "        \n",
    "        # drop column created_at\n",
    "        data = data.drop(columns=['created_at','customer_nk'])\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Orderlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: orderlines\n",
    "- Target Table: orderlines\n",
    "\n",
    "| Source Field  | Target Field  | Transformation Rule                                                                     |\n",
    "|---------------|---------------|-----------------------------------------------------------------------------------------|\n",
    "| orderlineid  | orderline_nk  | Direct Mapping                                                                          |\n",
    "| -             | orderline_id  | Auto Generated using `uuid_generate_v4()`                                               |\n",
    "| orderid       | order_id      | Lookup `order_id` from `orders` table based on `orderid`                                |\n",
    "| prod_id       | product_id    | Lookup `product_id` from `products` table based on `prod_id`                            |\n",
    "| quantity      | quantity      | Direct Mapping                                                                          |\n",
    "| orderdate     | order_date    | Direct Mapping                                                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_orderlines(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data orderlines from staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column orderlines\n",
    "        data = data.rename(columns={'orderlineid':'orderline_nk', 'orderid':'order_nk', 'prod_id':'product_nk', \n",
    "                                    'quantity':'quantity', 'orderdate':'order_date'})\n",
    "        \n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # Extract data from the `product` table\n",
    "        products = extract_target('products')\n",
    "\n",
    "        # Lookup `product_id` from `product` table based on `prod_id`   \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: products.loc[products['product_nk'] == x, 'product_id'].values[0])\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        data = data.drop(columns=['created_at','order_nk','product_nk'])\n",
    "\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table customer_orders_history\n",
    "Target:\n",
    "- Table customers\n",
    "- Table product\n",
    "- Table orders\n",
    "- Table orderlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: customer_orders_history\n",
    "- Target Table: customers\n",
    "\n",
    "| Source Field              | Target Field          | Transformation Rule                                                                  |\n",
    "|---------------------------|-----------------------|--------------------------------------------------------------------------------------|\n",
    "| customer_id               | customer_nk           | Direct Mapping                                                                       |\n",
    "| -                         | customer_id           | Auto Generated using `uuid_generate_v4()`                                            |\n",
    "| customer_firstname        | first_name            | Direct Mapping                                                                       |\n",
    "| customer_lastname         | last_name             | Direct Mapping                                                                       |\n",
    "| customer_address1         | address1              | Direct Mapping                                                                       |\n",
    "| customer_address2         | address2              | Direct Mapping                                                                       |\n",
    "| customer_city             | city                  | Direct Mapping                                                                       |\n",
    "| customer_state            | state                 | Direct Mapping                                                                       |\n",
    "| customer_zip              | zip                   | Direct Mapping                                                                       |\n",
    "| customer_country          | country               | Direct Mapping                                                                       |\n",
    "| customer_region           | region                | Direct Mapping                                                                       |\n",
    "| customer_email            | email                 | Direct Mapping                                                                       |\n",
    "| customer_phone            | phone                 | Direct Mapping                                                                       |\n",
    "| customer_creditcardtype   | credit_card_type      | Direct Mapping and Masking                                                                       |\n",
    "| customer_creditcard       | credit_card           | Direct Mapping                                                                       |\n",
    "| customer_creditcardexpiration | credit_card_expiration | Direct Mapping                                                                   |\n",
    "| customer_username         | username              | Direct Mapping                                                                       |\n",
    "| customer_password         | password              | Direct Mapping                                                                       |\n",
    "| customer_age              | age                   | Direct Mapping                                                                       |\n",
    "| customer_income           | income                | Direct Mapping                                                                       |\n",
    "| customer_gender           | gender                | Direct Mapping                                                                       |\n",
    "\n",
    "- Source Table: customer_orders_history\n",
    "- Target Table: products\n",
    "\n",
    "| Source Field              | Target Field          | Transformation Rule                                                                  |\n",
    "|---------------------------|-----------------------|--------------------------------------------------------------------------------------|\n",
    "| product_id                | product_nk            | Direct Mapping                                                                       |\n",
    "| -                         | product_id            | Auto Generated using `uuid_generate_v4()`                                            |\n",
    "| product_category          | category_id           | Lookup `category_id` from `categories` table based on `product_category`             |\n",
    "| product_title             | title                 | Direct Mapping                                                                       |\n",
    "| product_actor             | actor                 | Direct Mapping                                                                       |\n",
    "| product_price             | price                 | Direct Mapping                                                                       |\n",
    "| product_special           | special               | Direct Mapping                                                                       |\n",
    "| product_common_prod_id    | common_prod_id        | Direct Mapping                                                                       |\n",
    "\n",
    "- Source Table: customer_orders_history\n",
    "- Target Table: orders\n",
    "\n",
    "| Source Field              | Target Field          | Transformation Rule                                                                  |\n",
    "|---------------------------|-----------------------|--------------------------------------------------------------------------------------|\n",
    "| order_id                  | order_nk              | Direct Mapping                                                                       |\n",
    "| -                         | order_id              | Auto Generated using `uuid_generate_v4()`                                            |\n",
    "| order_customerid          | customer_id           | Lookup `customer_id` from `customers` table based on `order_customerid`              |\n",
    "| order_date                | order_date            | Direct Mapping                                                                       |\n",
    "| order_netamount           | net_amount            | Direct Mapping                                                                       |\n",
    "| order_tax                 | tax                   | Direct Mapping                                                                       |\n",
    "| order_totalamount         | total_amount          | Direct Mapping                                                                       |\n",
    "\n",
    "- Source Table: customer_orders_history\n",
    "- Target Table: orderlines\n",
    "\n",
    "| Source Field              | Target Field          | Transformation Rule                                                                  |\n",
    "|---------------------------|-----------------------|--------------------------------------------------------------------------------------|\n",
    "| orderline_id              | orderline_nk          | Direct Mapping                                                                       |\n",
    "| -                         | orderline_id          | Auto Generated using `uuid_generate_v4()`                                            |\n",
    "| order_id                  | order_id              | Lookup `order_id` from `orders` table based on `order_id`                            |\n",
    "| product_id                | product_id            | Lookup `product_id` from `products` table based on `product_id`                      |\n",
    "| orderline_quantity        | quantity              | Direct Mapping                                                                       |\n",
    "| orderline_orderdate       | order_date            | Direct Mapping                                                                       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_cust(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data customer from customer_order_hist staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column for customers\n",
    "        data = data.rename(columns={\n",
    "                                'customer_id': 'customer_nk',\n",
    "                                'customer_firstname': 'first_name',\n",
    "                                'customer_lastname': 'last_name',\n",
    "                                'customer_address1': 'address1',\n",
    "                                'customer_address2': 'address2',\n",
    "                                'customer_city': 'city',\n",
    "                                'customer_state': 'state',\n",
    "                                'customer_zip': 'zip',\n",
    "                                'customer_country': 'country',\n",
    "                                'customer_region': 'region',\n",
    "                                'customer_email': 'email',\n",
    "                                'customer_phone': 'phone',\n",
    "                                'customer_creditcardtype': 'credit_card_type',\n",
    "                                'customer_creditcard': 'credit_card',\n",
    "                                'customer_creditcardexpiration': 'credit_card_expiration',\n",
    "                                'customer_username': 'username',\n",
    "                                'customer_password': 'password',\n",
    "                                'customer_age': 'age',\n",
    "                                'customer_income': 'income',\n",
    "                                'customer_gender': 'gender'\n",
    "                            }) \n",
    "        \n",
    "        columns_to_keep = [\n",
    "            'customer_nk', 'customer_id', 'first_name', 'last_name', \n",
    "            'address1', 'address2', 'city', 'state', 'zip', \n",
    "            'country', 'region', 'email', 'phone', \n",
    "            'credit_card_type', 'credit_card', 'credit_card_expiration', \n",
    "            'username', 'password', 'age', 'income', 'gender'\n",
    "        ]\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=[col for col in data.columns if col not in columns_to_keep])\n",
    "\n",
    "        # Deduplication based on customer_nk\n",
    "        data = data.drop_duplicates(subset='customer_nk')\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_prod(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data orders from customer_order_hist staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column for products\n",
    "        data = data.rename(columns={\n",
    "            'product_id': 'product_nk', \n",
    "            'product_category': 'category_nk', \n",
    "            'product_title': 'title', \n",
    "            'product_actor': 'actor', \n",
    "            'product_price': 'price', \n",
    "            'product_special': 'special', \n",
    "            'product_common_prod_id': 'common_prod_id'\n",
    "        })\n",
    "\n",
    "        # Deduplication based on product_nk\n",
    "        data = data.drop_duplicates(subset='product_nk')\n",
    "\n",
    "        # Extract data from the `categories` table\n",
    "        categories = extract_target('categories')\n",
    "\n",
    "        #Lookup `category_id` from `categories` table based on `category`   \n",
    "        data['category_id'] = data['category_nk'].apply(lambda x: categories.loc[categories['category_nk'] == x, 'category_id'].values[0])\n",
    "        \n",
    "        # Get relevant columns\n",
    "        data = data[['product_nk', 'category_id', 'title', 'actor', 'price', 'special', 'common_prod_id']]\n",
    "\n",
    "\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_order(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data order from customer_order_hist staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column for orders\n",
    "        data = data.rename(columns={\n",
    "                    'order_id': 'order_nk', \n",
    "                    'order_customerid': 'customer_nk', \n",
    "                    'order_date': 'order_date', \n",
    "                    'order_netamount': 'net_amount', \n",
    "                    'order_tax': 'tax', \n",
    "                    'order_totalamount': 'total_amount'\n",
    "                })\n",
    "\n",
    "\n",
    "        # Deduplication based on order_nk\n",
    "        data = data.drop_duplicates(subset='order_nk')\n",
    "\n",
    "        # Extract data from the `customers` table\n",
    "        customer = extract_target('customers')\n",
    "\n",
    "        #Lookup `customer_id` from `customers` table based on `customer_nk`   \n",
    "        data['customer_id'] = data['customer_nk'].apply(lambda x: customer.loc[customer['customer_nk'] == x, 'customer_id'].values[0])\n",
    "        \n",
    "        # Get relevant columns\n",
    "        data = data[['order_nk', 'customer_id', 'order_date', 'net_amount', 'tax', 'total_amount']]\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_hist_orderline(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data orderline from customer_order_hist staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        #drop column order_date\n",
    "        data = data.drop(columns=['order_date'])\n",
    "\n",
    "        # rename column for orders\n",
    "        data = data.rename(columns={\n",
    "            'orderline_id': 'orderline_nk', \n",
    "            'order_id': 'order_nk', \n",
    "            'product_id': 'product_nk', \n",
    "            'orderline_quantity': 'quantity', \n",
    "            'orderline_orderdate': 'order_date'\n",
    "        })\n",
    "\n",
    "        # Deduplication based on order_nk\n",
    "        data = data.drop_duplicates(subset=['orderline_nk','order_nk','product_nk','quantity'])\n",
    "\n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # Extract data from the `product` table\n",
    "        products = extract_target('products')\n",
    "\n",
    "        # Lookup `product_id` from `product` table based on `prod_id`   \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: products.loc[products['product_nk'] == x, 'product_id'].values[0])\n",
    "        \n",
    "        \n",
    "        # Get relevant columns\n",
    "        data = data[['orderline_nk', 'order_id', 'product_id', 'quantity', 'order_date']]\n",
    "        \n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Customer History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: cust_hist\n",
    "- Target Table: cust_hist\n",
    "\n",
    "| Source Field | Target Field   | Transformation Rule                                                                 |\n",
    "|--------------|----------------|-------------------------------------------------------------------------------------|\n",
    "| customerid   | customer_id    | Lookup `customer_id` from `customers` table based on `customerid`                   |\n",
    "| orderid      | order_id       | Lookup `order_id` from `orders` table based on `orderid`                             |\n",
    "| prod_id      | product_id     | Lookup `product_id` from `products` table based on `prod_id`                         |\n",
    "| created_at   | created_at     | Direct Mapping                                                                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cust_hist(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data cust_hist from staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column orderlines\n",
    "        data = data.rename(columns={'customerid':'customer_nk', 'prod_id':'product_nk', 'orderid':'order_nk'})\n",
    "        \n",
    "        # Extract data from the `customers` table\n",
    "        customers = extract_target('customers')\n",
    "\n",
    "        # Lookup `customer_id` from `customers` table based on `customerid`   \n",
    "        data['customer_id'] = data['customer_nk'].apply(lambda x: customers.loc[customers['customer_nk'] == x, 'customer_id'].values[0])\n",
    "        \n",
    "\n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # Extract data from the `product` table\n",
    "        products = extract_target('products')\n",
    "\n",
    "        # Lookup `product_id` from `product` table based on `prod_id`   \n",
    "        data['product_id'] = data['product_nk'].apply(lambda x: products.loc[products['product_nk'] == x, 'product_id'].values[0])\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        data = data.drop(columns=['customer_nk','order_nk','product_nk'])\n",
    "\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table order_status_analytic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source Table: order_status_analytic\n",
    "- Target Table: order_status_analytic\n",
    "\n",
    "| Source Field | Target Field     | Transformation Rule                                  |\n",
    "|--------------|------------------|------------------------------------------------------|\n",
    "| orderid      | order_nk         | Direct Mapping                                       |\n",
    "| -            | order_id         | Auto Generated using `uuid_generate_v4()`            |\n",
    "| sum_stock    | sum_stock        | Direct Mapping                                       |\n",
    "| status       | status           | Direct Mapping                                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_order_status_analytic(data: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to transform data order_status_analytic from staging database to the data warehouse.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = \"transformation\"\n",
    "        # rename column order_status_analytic\n",
    "        data = data.rename(columns={'orderid':'order_nk'})\n",
    "\n",
    "        # Extract data from the `orders` table\n",
    "        orders = extract_target('orders')\n",
    "\n",
    "        # Lookup `order_id` from `orders` table based on `orderid`   \n",
    "        data['order_id'] = data['order_nk'].apply(lambda x: orders.loc[orders['order_nk'] == x, 'order_id'].values[0])\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        data = data.drop(columns='created_at')\n",
    "\n",
    "        log_msg = {\n",
    "                \"step\" : \"warehouse\",\n",
    "                \"process\": process,\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"process\": process,\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "            }\n",
    "        print(e)\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name, process=process)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        # Save the log message\n",
    "        etl_log(log_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Customer Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Customer Table Validation:\n",
    "    - Validate that email addresses conform to standard formats (e.g., yahoo.com, hotmail.com, gmail.com).\n",
    "    - Ensure that the phone number contains exactly 10 digits.\n",
    "    - Validate the credit card expiration date format to be in the YYYY/MM format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation email domain\n",
    "def validate_email_format(email):\n",
    "    email_regex = re.compile(r\"^[\\w\\.-]+@(yahoo\\.com|hotmail\\.com|gmail\\.com)$\")\n",
    "    return bool(email_regex.match(email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure phone number contains 10 digits\n",
    "def validate_phone_format(phone):\n",
    "    phone_regex = re.compile(r\"^\\d{10}$\")\n",
    "    return bool(phone_regex.match(phone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate credit card number expiration date format is YYYY/MM\n",
    "def validate_credit_card_expiration_format(expiration_date):\n",
    "    expiration_date_regex = re.compile(r\"^\\d{4}/\\d{2}$\")\n",
    "    return bool(expiration_date_regex.match(expiration_date))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Product Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Products Table Validation\n",
    "    - Ensure that the price value is within the range of 0 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the price value is within the range of 0 to 100.\n",
    "def validate_price_range(price):\n",
    "    return 0 <= price <= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Orders and Orderlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Orders Table Validation:\n",
    "    - Ensure that net_amount, tax, and total_amount are positive values.\n",
    "- Orderlines Table Validation\n",
    "    - Ensure that quantity is a positive number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that net_amount, tax, and total_amount are positive values.\n",
    "def validate_positive_value(value):\n",
    "    return value >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table order_status_analytic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- order_status_analytic Table Validation\n",
    "    - Validate that the status is either partial, fulfilled, or backordered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Validate that the status is either partial, fulfilled, or backordered.\n",
    "def validate_order_status(status):\n",
    "    return status in ['partial', 'fulfilled', 'backordered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_data(data: pd.DataFrame, table_name: str, validation_functions: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to validate data based on the specified validation functions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a report DataFrame\n",
    "        report_data = {f'validate_{name}': data[name].apply(func) for name, func in validation_functions.items()}\n",
    "        report_df = pd.DataFrame(report_data)\n",
    "\n",
    "        # Summarize status data by all conditions\n",
    "        report_df['all_valid'] = report_df.all(axis=1)\n",
    "\n",
    "        # Filter out valid rows (all_valid = 'True')\n",
    "        valid_data_df = data[report_df['all_valid']]\n",
    "\n",
    "        # Filter out invalid rows (all_valid = 'False')\n",
    "        invalid_data_df = data[~report_df['all_valid']]\n",
    "\n",
    "        # Create success log message\n",
    "        log_msg = {\n",
    "            \"step\": \"warehouse\",\n",
    "            \"process\": \"validation\",\n",
    "            \"status\": \"success\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "        return valid_data_df, invalid_data_df\n",
    "    except Exception as e:\n",
    "        # Create fail log message\n",
    "        log_msg = {\n",
    "            \"step\": \"warehouse\",\n",
    "            \"process\": \"validation\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"staging\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp,\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "    finally:\n",
    "        etl_log(log_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Categories\n",
    "df_category = extract_staging(table_name = \"categories\", schema_name = DB_SHCHEMA_STG)\n",
    "category_tf = transform_categories(data = df_category, table_name = \"categories\")\n",
    "load_warehouse(data = category_tf, schema = \"public\", table_name = \"categories\",\n",
    "                idx_name = \"category_nk\", source = \"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Customer\n",
    "df_customer = extract_staging(table_name=\"customers\", schema_name=DB_SHCHEMA_STG)\n",
    "customer_tf = transform_customer(data=df_customer, table_name=\"customers\")\n",
    "valid_cust, invalid_cust = validation_data(data=customer_tf, table_name=\"customers\", validation_functions={\"email\": validate_email_format, \n",
    "                                                                                                           \"phone\": validate_phone_format, \n",
    "                                                                                                           \"credit_card_expiration\": validate_credit_card_expiration_format})\n",
    "load_warehouse(data=valid_cust, schema=\"public\", table_name=\"customers\",\n",
    "                idx_name=\"customer_nk\", source=\"staging\")\n",
    "if (not invalid_cust.empty):\n",
    "    handle_error(data=invalid_cust, bucket_name='error-dellstore', table_name=\"customers\", process='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Product\n",
    "df_product = extract_staging(table_name=\"products\", schema_name=DB_SHCHEMA_STG)\n",
    "product_tf = transform_product(data=df_product, table_name=\"products\")\n",
    "valid_product, invalid_product = validation_data(data=product_tf, table_name=\"products\", validation_functions={\"price\": validate_price_range})\n",
    "load_warehouse(data=valid_product, schema=\"public\", table_name=\"products\", \n",
    "               idx_name=\"product_nk\", source=\"staging\")\n",
    "if (not invalid_product.empty):\n",
    "    handle_error(data=invalid_product, bucket_name='error-dellstore', table_name=\"products\", process='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Inventory\n",
    "df_inventory = extract_staging(table_name=\"inventory\", schema_name=DB_SHCHEMA_STG)\n",
    "inventory_tf = transform_inventory(data=df_inventory, table_name=\"inventory\")\n",
    "load_warehouse(data=inventory_tf, schema=\"public\", table_name=\"inventory\", idx_name=\"product_nk\", source=\"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Orders\n",
    "df_orders = extract_staging(table_name=\"orders\", schema_name=DB_SHCHEMA_STG)\n",
    "orders_tf = transform_orders(data=df_orders, table_name=\"orders\")\n",
    "valid_orders, invalid_orders = validation_data(data=orders_tf, table_name=\"orders\", validation_functions={\"net_amount\": validate_positive_value, \n",
    "                                                                                                           \"tax\": validate_positive_value, \n",
    "                                                                                                           \"total_amount\": validate_positive_value})\n",
    "load_warehouse(data=valid_orders, schema=\"public\", table_name=\"orders\",\n",
    "                idx_name=\"order_nk\", source=\"staging\")\n",
    "if (not invalid_orders.empty):\n",
    "    handle_error(data=invalid_orders, bucket_name='error-dellstore', table_name=\"orders\", process='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Orderlines\n",
    "df_orderlines = extract_staging(table_name=\"orderlines\", schema_name=DB_SHCHEMA_STG)\n",
    "orderlines_tf = transform_orderlines(data=df_orderlines, table_name=\"orderlines\")\n",
    "valid_orderlines, invalid_orderlines = validation_data(data=orderlines_tf, table_name=\"orderlines\", validation_functions={\"quantity\": validate_positive_value})\n",
    "load_warehouse(data=valid_orderlines, schema=\"public\", \n",
    "               table_name=\"orderlines\", idx_name=[\"orderline_nk\",\"order_id\",\"product_id\",\"quantity\"], source=\"staging\")\n",
    "if (not invalid_orderlines.empty):\n",
    "    handle_error(data=invalid_orderlines, bucket_name='error-dellstore', table_name=\"orderlines\", process='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data customer_orders_history\n",
    "df_order_hist = extract_staging(table_name=\"customer_orders_history\", schema_name=DB_SHCHEMA_STG)\n",
    "\n",
    "# Data Customer\n",
    "cust_order_hist_tf = transform_order_hist_cust(data=df_order_hist, table_name=\"customer_orders_history\")\n",
    "valid_cust_hist, invalid_cust_hist = validation_data(data=cust_order_hist_tf, table_name=\"customers\", validation_functions={\"email\": validate_email_format, \n",
    "                                                                                                           \"phone\": validate_phone_format, \n",
    "                                                                                                           \"credit_card_expiration\": validate_credit_card_expiration_format})\n",
    "load_warehouse(data=valid_cust_hist, schema=\"public\", table_name=\"customers\", idx_name=[\"customer_nk\"], source=\"staging\")\n",
    "if (not invalid_cust_hist.empty):\n",
    "    handle_error(data=invalid_cust_hist, bucket_name='error-dellstore', table_name=\"customers\", process='validation')\n",
    "\n",
    "# Data Product\n",
    "prod_order_hist_tf = transform_order_hist_prod(data=df_order_hist, table_name=\"customer_orders_history\")\n",
    "valid_order_hist_prod, invalid_order_hist_prod = validation_data(data=prod_order_hist_tf, table_name=\"products\", validation_functions={\"price\": validate_price_range})\n",
    "load_warehouse(data=valid_order_hist_prod, schema=\"public\", table_name=\"products\", idx_name=[\"product_nk\"], source=\"staging\")\n",
    "if (not invalid_order_hist_prod.empty):\n",
    "    handle_error(data=invalid_order_hist_prod, bucket_name='error-dellstore', table_name=\"products\", process='validation')\n",
    "\n",
    "# Data Orders\n",
    "order_hist_tf = transform_order_hist_order(data=df_order_hist, table_name=\"customer_orders_history\")\n",
    "valid_order_hist, invalid_order_hist = validation_data(data=order_hist_tf, table_name=\"orders\", validation_functions={\"net_amount\": validate_positive_value, \n",
    "                                                                                                           \"tax\": validate_positive_value, \n",
    "                                                                                                           \"total_amount\": validate_positive_value})\n",
    "load_warehouse(data=valid_order_hist, schema=\"public\", table_name=\"orders\", idx_name=[\"order_nk\"], source=\"staging\")\n",
    "if (not invalid_order_hist.empty):\n",
    "    handle_error(data=invalid_order_hist, bucket_name='error-dellstore', table_name=\"orders\", process='validation')\n",
    "\n",
    "# Data Orderlines\n",
    "orderline_hist_tf = transform_order_hist_orderline(data=df_order_hist, table_name=\"customer_orders_history\")\n",
    "valid_orderline_hist, invalid_orderline_hist = validation_data(data=orderline_hist_tf, table_name=\"orderlines\", validation_functions={\"quantity\": validate_positive_value})\n",
    "load_warehouse(data=valid_orderline_hist, schema=\"public\", table_name=\"orderlines\", \n",
    "               idx_name=[\"orderline_nk\",\"order_id\",\"product_id\",\"quantity\"], source=\"staging\")\n",
    "if (not invalid_orderline_hist.empty):\n",
    "    handle_error(data=invalid_orderline_hist, bucket_name='error-dellstore', table_name=\"orderlines\", process='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Customer History\n",
    "df_cust_hist = extract_staging(table_name=\"cust_hist\", schema_name=DB_SHCHEMA_STG)\n",
    "cust_hist_tf = transform_cust_hist(data=df_cust_hist, table_name=\"cust_hist\")\n",
    "load_warehouse(data=cust_hist_tf, schema=\"public\", table_name=\"cust_hist\", \n",
    "               idx_name=[\"customer_id\",\"order_id\",\"product_id\"], source=\"staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Order Status Analytic\n",
    "df_order_analytic = extract_staging(table_name=\"order_status_analytic\", schema_name=DB_SHCHEMA_STG)\n",
    "order_analytic_tf = transform_order_status_analytic(data=df_order_analytic, table_name=\"order_status_analytic\")\n",
    "valid_orders_analytic, invalid_orders_analytic = validation_data(data=order_analytic_tf, table_name=\"order_status_analytic\", validation_functions={\"status\": validate_order_status})\n",
    "load_warehouse(data=valid_orders_analytic, schema=\"public\", table_name=\"order_status_analytic\", \n",
    "               idx_name=\"order_id\", source=\"staging\")\n",
    "if (not invalid_orders_analytic.empty):\n",
    "    handle_error(data=invalid_orders_analytic, bucket_name='error-dellstore', table_name=\"order_status_analytic\", process='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modularize the functions mentioned earlier into separate modules for better management and organization. Here is a suggested structure:\n",
    "\n",
    "```\n",
    "project\n",
    "   README.md\n",
    "   .env\n",
    "   pipeline_staging.py    \n",
    "src\n",
    "   log\n",
    "   |       log.py\n",
    "   staging\n",
    "       extract\n",
    "             extract_db.py\n",
    "             extract_api.py\n",
    "             extract_spreadsheet.py\n",
    "       load\n",
    "             load_minio.py\n",
    "             load_staging.py\n",
    "       models\n",
    "             customers.sql\n",
    "             products.sql\n",
    "   warehouse\n",
    "       extract\n",
    "             extract_staging.py\n",
    "       load\n",
    "             load_minio.py\n",
    "             load_warehouse.py\n",
    "       models\n",
    "             log.sql\n",
    "       transform\n",
    "             products.py\n",
    "             customers.py\n",
    "       validation\n",
    "             validation_data.py\n",
    "creds\n",
    "      data-pipeline.json\n",
    "pipeline_staging.py\n",
    "pipeline_warehouse.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link git repository: [git repository](https://github.com/Kurikulum-Sekolah-Pacmann/data_pipeline_dellstore.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
