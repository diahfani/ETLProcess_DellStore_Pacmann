{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week #3 - Live Class\n",
    "Data Pipeline Course - Sekolah Engineer - Pacmann Academy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data Ingestion` emphasizes pulling data from various sources (`Extract`) and directing it to targets (`Load`).\n",
    "- `Data Extraction` involves retrieving data from various sources\n",
    "- `Data Loading` involves transferring this data into target storage systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideration when carrying out Ingestion\n",
    "- What’s the format? (Unstructured, Semi-structured, Structured)\n",
    "- What’s the frequency? (Batch, Micro-Batch, Streaming)\n",
    "- Is the data ingestion process synchronous or asynchronous?\n",
    "- What is the expected data volume?\n",
    "- What measures are in place to ensure data reliability during ingestion?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Full Extraction: Extracting all the data from the source system\n",
    "- Full Load: Load all the data to destination system\n",
    "- Incremental Extraction: Extracting only the new data or data that has changed since the last extraction\n",
    "- Incremental Load: Load only new or updated data at regular intervals, rather than moving all data at once\n",
    "\n",
    "<img src='pict/live_w3_01.png' width=\"800\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Case: Dell DVD Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Description\n",
    "\n",
    "`Problem`\n",
    "\n",
    "The Dell DVD Store is facing challenges with its current data processing. The store needs to handle data from multiple sources such as spreadsheets, databases, and APIs. The key challenges include:\n",
    "- Database: The Dell DVD Store saves data from current system.\n",
    "- API: Retrieves data from the old system and contains historical data from the old system.\n",
    "- Spreadsheet: Contains analysis results from the team about order status based on the current product stock.\n",
    "\n",
    "<img src='pict/live_w3_03.png' width=\"800\"> <br>\n",
    "\n",
    "`Solution`\n",
    "\n",
    "To address these challenges, we propose creating a comprehensive data pipeline for the Dell DVD Store. This pipeline will involve the following steps:\n",
    "- Data Extraction:\n",
    "Sources: Extract data from spreadsheets, databases, and APIs.\n",
    "Techniques: Use both full and incremental extraction methods to retrieve data efficiently.\n",
    "- Data Load:\n",
    "Staging: Load raw data into a staging database (PostgreSQL) without transformation.\n",
    "Final Load: Transfer clean and transformed data to the final destination.\n",
    "Failure Handling: Log failed data loads to MinIO object storage for reprocessing\n",
    "- Data Transformation:\n",
    "Cleaning: Handle missing values, incorrect data formats, and other data quality issues.\n",
    "Trasnforming: Add derived fields and calculated metrics as needed.\n",
    "\n",
    "`Tools and Technologies`:\n",
    "- Python: For build Data Pipeline\n",
    "- PostgreSQL: For log, staging and final data storage.\n",
    "- MinIO: For load failed data.\n",
    "- Docker: For running MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "1. Restore Database Dell DVD Store [Link](https://drive.google.com/file/d/1jhbrIs2bVlVyDCc1CK__nyttFB1Ia4rM/view?usp=sharing)\n",
    "2. Dupplicate Spreadsheet [Link](https://docs.google.com/spreadsheets/d/1qEekzxEExv_PZpT3LFg5HPllCH2OiKX1wwESJmlAZV0/edit?usp=sharing)\n",
    "3. Check Data API [Link](https://api-history-order.vercel.app/api/dummydata?start_date=2004-01-01&end_date=2004-01-01)\n",
    "\n",
    "Database\n",
    "1. Staging Schema [Link](https://drive.google.com/file/d/1jmO30jlWffX9SmaDZWxuINUeglmFkp0_/view?usp=sharing)\n",
    "2. Log Schema [Link](https://drive.google.com/file/d/1BTeIeGQGXnS6zT4GqCJxUpcWAZIq_Jxb/view?usp=sharing)\n",
    "\n",
    "\n",
    "For Porject\n",
    "1. Save Your Credential Google Service Account\n",
    "2. Prepare Your MiniO (Access Key, Secreet Key, Bucket Name: \"error-dellstore\")\n",
    "3. Create Your Database (log and staging)\n",
    "4. create your .env\n",
    "\n",
    "    ```\n",
    "    DB_HOST=\"localhost\"\n",
    "    DB_USER=\"YOUR POSTGRES USER\"\n",
    "    DB_PASS=\"YOUR POSGRES PASS\"\n",
    "\n",
    "    DB_NAME_SOURCE=\"dellstore\"\n",
    "    DB_NAME_STG=\"staging\"\n",
    "    DB_SHCHEMA_STG=\"staging\"\n",
    "    DB_NAME_log=\"etl_log\"\n",
    "\n",
    "    CRED_PATH='YOUR_PATH/creds/data-pipeline-427506-50d868a444ee.json'\n",
    "    MODEL_PATH='YOUR_PATH/models/'\n",
    "    KEY_SPREADSHEET=\"YOUR SPREADSHEET KEY\"\n",
    "\n",
    "    ACCESS_KEY_MINIO = 'YOUR MINIO ACCESS KEY'\n",
    "    SECRET_KEY_MINIO = 'YOUR MINIO SECRET KEY'\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'oauth2client'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moauth2client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservice_account\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ServiceAccountCredentials\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgspread\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'oauth2client'"
     ]
    }
   ],
   "source": [
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "import requests\n",
    "from pangres import upsert\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load your file .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "\n",
    "DB_NAME_SOURCE = os.getenv(\"DB_NAME_SOURCE\")\n",
    "DB_NAME_STG = os.getenv(\"DB_NAME_STG\")\n",
    "DB_SHCHEMA_STG = os.getenv(\"DB_SHCHEMA_STG\")\n",
    "DB_NAME_log = os.getenv(\"DB_NAME_log\")\n",
    "\n",
    "\n",
    "CRED_PATH = os.getenv(\"CRED_PATH\")\n",
    "KEY_SPREADSHEET = os.getenv(\"KEY_SPREADSHEET\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "ACCESS_KEY_MINIO = os.getenv(\"ACCESS_KEY_MINIO\")\n",
    "SECRET_KEY_MINIO = os.getenv(\"SECRET_KEY_MINIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage SQL queries efficiently using external `.sql` files, you can create a function that reads these files and returns their content.<br>\n",
    "Each `.sql` file should contain the SQL query for the respective table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql(table_name):\n",
    "    #open your file .sql\n",
    "    with open(f\"{MODEL_PATH}{table_name}.sql\", 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    #return query text\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Log Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A log is a record of events that occur during the execution of a data pipeline. It captures essential information about the processes and their status, making it easier to monitor, debug, and audit the pipeline operations \n",
    "\n",
    "The etl_log function is designed to log ETL (Extract, Transform, Load) operations into a PostgreSQL database.\n",
    "\n",
    "Your Log Message: <br>\n",
    "<code>\n",
    "log_msg = { <br>\n",
    "            \"step\": \"staging | warehouse\"\n",
    "            \"process\" : \"extraction | transformation | load\", <br>\n",
    "            \"status\": \"success | failed\", <br>\n",
    "            \"source\": \"spreadsheet | database | api\", <br>\n",
    "            \"table_name\": \"worksheet_name | table_name\", <br>\n",
    "            \"etl_date\": \"Current timestamp\" <br>\n",
    "            \"error_msg\": \"Error Message when error occur\" <br>\n",
    "        }\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_log(log_msg: dict):\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_log}\")\n",
    "        \n",
    "        # convert dictionary to dataframe\n",
    "        df_log = pd.DataFrame([log_msg])\n",
    "\n",
    "        #extract data log\n",
    "        df_log.to_sql(name = \"etl_log\",  # Your log table\n",
    "                        con = conn,\n",
    "                        if_exists = \"append\",\n",
    "                        index = False)\n",
    "    except Exception as e:\n",
    "        print(\"Can't save your log message. Cause: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = sqlalchemy.text(read_sql(\"log\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuction read_etl_log untuk membaca informasi log dari tabel log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_etl_log(filter_params: dict):\n",
    "    \"\"\"\n",
    "    function read_etl_log that reads log information from the etl_log table and extracts the maximum etl_date for a specific process, step, table name, and status.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_log}\")\n",
    "        \n",
    "        # To help with the incremental process, get the etl_date from the relevant process\n",
    "        \"\"\"\n",
    "        SELECT MAX(etl_date)\n",
    "        FROM etl_log \"\n",
    "        WHERE \n",
    "            step = %s and\n",
    "            table_name ilike %s and\n",
    "            status = %s and\n",
    "            process = %s\n",
    "        \"\"\"\n",
    "        query = sqlalchemy.text(read_sql(\"log\"))\n",
    "\n",
    "        # Execute the query with pd.read_sql\n",
    "        df = pd.read_sql(sql=query, con=conn, params=(filter_params,))\n",
    "\n",
    "        #return extracted data\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Can't execute your query. Cause: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extract Data From Database\n",
    "\n",
    "Extract Data From PostgreSQL\n",
    "\n",
    "- Full Extraction: Initial Load\n",
    "- Incremental Extraction: Get new and updated data\n",
    "\n",
    "Function Steps:\n",
    "1. Establish Database Connection: Connects to a PostgreSQL database named dellstore.\n",
    "2. Read Log Data: Reads existing log data from log.csv to determine the last successful extraction timestamp (etl_date).\n",
    "3. Initial Load or Incremental Extraction:\n",
    "    - If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "    - Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "4. Query Execution: Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "5. Data Extraction: Executes the SQL query using pd.read_sql to fetch the data into a Pandas DataFrame (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_database(table_name: str): \n",
    "    \n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_SOURCE}\")\n",
    "\n",
    "        # Get date from previous process\n",
    "        filter_log = {\"step_name\": \"staging\",\n",
    "                    \"table_name\": table_name,\n",
    "                    \"status\": \"success\",\n",
    "                    \"process\": \"load\"}\n",
    "        etl_date = read_etl_log(filter_log)\n",
    "\n",
    "\n",
    "        # If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "        # Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "        if(etl_date['max'][0] == None):\n",
    "            etl_date = '1111-01-01'\n",
    "        else:\n",
    "            etl_date = etl_date[max][0]\n",
    "            # etl_date = etl_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM customers \n",
    "        WHERE created_at > :etl_date\n",
    "        \"\"\"\n",
    "        query = sqlalchemy.text(read_sql(table_name))\n",
    "\n",
    "        #Execute the query with pd.read_sql\n",
    "        df = pd.read_sql(sql=query, con=conn, params=({\"etl_date\":(etl_date)},))\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"database\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"process\":\"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extract Data From API\n",
    "\n",
    "Extract Data From API (context: Old data from previous system)\n",
    "\n",
    "- Backfilling: Specify a date range in the date parameter to retrieve historical data\n",
    "\n",
    "Function Steps: \n",
    "1. Establish API endpoint: [Link](https://api-history-order.vercel.app)\n",
    "2. List of parameter API\n",
    "    - start_date\n",
    "    - end_date\n",
    "5. Data Extraction: Hit the endpoint API to fetch the data into a Pandas DataFrame (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(link_api:str, list_parameter:dict, data_name):\n",
    "    try:\n",
    "        # Establish connection to API\n",
    "        resp = requests.get(link_api, params=list_parameter)\n",
    "\n",
    "        # Parse the response JSON\n",
    "        raw_response = resp.json()\n",
    "\n",
    "        # Convert the JSON data to a pandas DataFrame\n",
    "        df_api = pd.DataFrame(raw_response)\n",
    "\n",
    "        # create success log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        return df_api\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while making the API request: {e}\")\n",
    "\n",
    "        # create fail log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred while parsing the response JSON: {e}\")\n",
    "\n",
    "        # create fail log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Extract Data From Spreadsheet\n",
    "\n",
    "Steps:\n",
    "1. Define the credentials needed to access the spreadsheet.\n",
    "2. Open file by key\n",
    "3. Retrieve data from a specific sheet.\n",
    "4. Perform Data Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function aims to define the credentials needed to access the spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_gspread():\n",
    "    scope = ['https://spreadsheets.google.com/feeds',\n",
    "             'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    #Define your credentials\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(CRED_PATH, scope) # Your json file here\n",
    "\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    return gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key_file is a unique identifier for the Google Sheets file.\n",
    "Access to the spreadsheet file is obtained using the key from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_key_file(key_file:str):\n",
    "    #define credentials to open the file\n",
    "    gc = auth_gspread()\n",
    "    \n",
    "    #open spreadsheet file by key\n",
    "    sheet_result = gc.open_by_key(key_file)\n",
    "    \n",
    "    return sheet_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extract_spreadsheet function is used to retrieve data from a specific sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sheet(key_file:str, worksheet_name: str) -> pd.DataFrame:\n",
    "    # init sheet\n",
    "    sheet_result = init_key_file(key_file)\n",
    "    \n",
    "    worksheet_result = sheet_result.worksheet(worksheet_name)\n",
    "    \n",
    "    df_result = pd.DataFrame(worksheet_result.get_all_values())\n",
    "    \n",
    "    # set first rows as columns\n",
    "    df_result.columns = df_result.iloc[0]\n",
    "    \n",
    "    # get all the rest of the values\n",
    "    df_result = df_result[1:].copy()\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spreadsheet(worksheet_name: str, key_file: str):\n",
    "\n",
    "    try:\n",
    "        # extract data\n",
    "        df_data = extract_sheet(worksheet_name = worksheet_name,\n",
    "                                    key_file = key_file)\n",
    "        df_data['created_at'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # success log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"spreadsheet\",\n",
    "                \"table_name\": worksheet_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "    except Exception as e:\n",
    "        # fail log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"extraction\",\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"spreadsheet\",\n",
    "                \"table_name\": worksheet_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "    finally:\n",
    "        # load log to database\n",
    "        etl_log(log_msg)\n",
    "        \n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Handle Failure Data\n",
    "\n",
    "we will learn how to handle any data failures processing by storing the failed data in object storage using MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Minio libray is used to interact with a MinIO server. \n",
    "from minio import Minio\n",
    "\n",
    "# BytesIO provides a way to work with binary data in memory as if it were a file.\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Function handle_error to dump failure data to MiniO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error(data, bucket_name:str, table_name:str):\n",
    "\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Initialize MinIO client\n",
    "    client = Minio('localhost:9000',\n",
    "                access_key=ACCESS_KEY_MINIO,\n",
    "                secret_key=SECRET_KEY_MINIO,\n",
    "                secure=False)\n",
    "\n",
    "    # Make a bucket if it doesn't exist\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "\n",
    "    # Convert DataFrame to CSV and then to bytes\n",
    "    csv_bytes = data.to_csv().encode('utf-8')\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    # Upload the CSV file to the bucket\n",
    "    client.put_object(\n",
    "        bucket_name=bucket_name,\n",
    "        object_name=f\"{table_name}_{current_date}.csv\", #name the fail source name and current etl date\n",
    "        data=csv_buffer,\n",
    "        length=len(csv_bytes),\n",
    "        content_type='application/csv'\n",
    "    )\n",
    "\n",
    "    # List objects in the bucket\n",
    "    objects = client.list_objects(bucket_name, recursive=True)\n",
    "    for obj in objects:\n",
    "        print(obj.object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load Data to Satging Area\n",
    "- Load raw data to staging area\n",
    "- strategy: apply upsert using libray pangres based on primary key for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staging(data, schema:str, table_name: str, idx_name:str, source):\n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME_STG}\")\n",
    "        \n",
    "        # set data index or primary key\n",
    "        data = data.set_index(idx_name)\n",
    "        \n",
    "        # Do upsert (Update for existing data and Insert for new data)\n",
    "        upsert(con = conn,\n",
    "                df = data,\n",
    "                table_name = table_name,\n",
    "                schema = schema,\n",
    "                if_row_exists = \"update\")\n",
    "        \n",
    "        #create success log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"staging\",\n",
    "                \"process\":\"load\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": source,\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        # return data\n",
    "    except Exception as e:\n",
    "\n",
    "        #create fail log message\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"process\":\"load\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") , # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "        # Handling error: save data to Object Storage\n",
    "        try:\n",
    "            handle_error(data = data, bucket_name='error-dellstore', table_name= table_name)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    finally:\n",
    "        etl_log(log_msg)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='pict/live_w3_04.png' width=\"800\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Ingestion Dell DVD Store Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema Source: <br>\n",
    "<img src='pict/live_w3_05.png' width=\"500\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "etl_date, df_customer = extract_database(table_name = \"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>address1</th>\n",
       "      <th>address2</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>...</th>\n",
       "      <th>phone</th>\n",
       "      <th>creditcardtype</th>\n",
       "      <th>creditcard</th>\n",
       "      <th>creditcardexpiration</th>\n",
       "      <th>username</th>\n",
       "      <th>password</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>gender</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>Becky</td>\n",
       "      <td>Cochran</td>\n",
       "      <td>193 Hailey Views\\nMichaelside, AS 48241</td>\n",
       "      <td>None</td>\n",
       "      <td>East Charleneshire</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>53868</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2415449050</td>\n",
       "      <td>5</td>\n",
       "      <td>6630987872369588</td>\n",
       "      <td>2010/03</td>\n",
       "      <td>beckycochran123</td>\n",
       "      <td>password</td>\n",
       "      <td>58</td>\n",
       "      <td>60000</td>\n",
       "      <td>M</td>\n",
       "      <td>2002-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>Raymond</td>\n",
       "      <td>Yang</td>\n",
       "      <td>683 Albert Ports\\nLake Waltershire, CO 77913</td>\n",
       "      <td>None</td>\n",
       "      <td>Laneberg</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>18452</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1896033667</td>\n",
       "      <td>2</td>\n",
       "      <td>3715867913328111</td>\n",
       "      <td>2011/10</td>\n",
       "      <td>raymondyang123</td>\n",
       "      <td>password</td>\n",
       "      <td>27</td>\n",
       "      <td>20000</td>\n",
       "      <td>F</td>\n",
       "      <td>2003-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>Melanie</td>\n",
       "      <td>Wade</td>\n",
       "      <td>514 Tonya Heights Suite 730\\nSouth Davidfurt, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Port Jessica</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>53356</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3029418206</td>\n",
       "      <td>5</td>\n",
       "      <td>3617457962129265</td>\n",
       "      <td>2009/11</td>\n",
       "      <td>melaniewade123</td>\n",
       "      <td>password</td>\n",
       "      <td>43</td>\n",
       "      <td>100000</td>\n",
       "      <td>M</td>\n",
       "      <td>2001-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>Heather</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>75935 Flynn Island Suite 933\\nSouth Alexis, MD...</td>\n",
       "      <td>None</td>\n",
       "      <td>Maryton</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>44395</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3748672054</td>\n",
       "      <td>4</td>\n",
       "      <td>3344003576319665</td>\n",
       "      <td>2011/07</td>\n",
       "      <td>heathercruz123</td>\n",
       "      <td>password</td>\n",
       "      <td>85</td>\n",
       "      <td>80000</td>\n",
       "      <td>M</td>\n",
       "      <td>2003-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>Heather</td>\n",
       "      <td>Burgess</td>\n",
       "      <td>6732 Brandi Trafficway Suite 104\\nNorth Jonath...</td>\n",
       "      <td>None</td>\n",
       "      <td>South Ryan</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>37471</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3354132892</td>\n",
       "      <td>4</td>\n",
       "      <td>8717996907886119</td>\n",
       "      <td>2008/05</td>\n",
       "      <td>heatherburgess123</td>\n",
       "      <td>password</td>\n",
       "      <td>66</td>\n",
       "      <td>100000</td>\n",
       "      <td>M</td>\n",
       "      <td>2000-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>19995</td>\n",
       "      <td>Marc</td>\n",
       "      <td>Gilbert</td>\n",
       "      <td>7502 Wilkins Knolls Suite 109\\nSouth Deanna, K...</td>\n",
       "      <td>None</td>\n",
       "      <td>Proctorton</td>\n",
       "      <td>Texas</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6088190939</td>\n",
       "      <td>1</td>\n",
       "      <td>7026820751838387</td>\n",
       "      <td>2011/08</td>\n",
       "      <td>marcgilbert123</td>\n",
       "      <td>password</td>\n",
       "      <td>20</td>\n",
       "      <td>100000</td>\n",
       "      <td>M</td>\n",
       "      <td>2003-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19996</td>\n",
       "      <td>Sandra</td>\n",
       "      <td>Ochoa</td>\n",
       "      <td>45384 James View Apt. 503\\nNew David, PA 63231</td>\n",
       "      <td>None</td>\n",
       "      <td>Stevenshire</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>0</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>5392978326</td>\n",
       "      <td>5</td>\n",
       "      <td>9188167935237288</td>\n",
       "      <td>2008/11</td>\n",
       "      <td>sandraochoa123</td>\n",
       "      <td>password</td>\n",
       "      <td>78</td>\n",
       "      <td>60000</td>\n",
       "      <td>F</td>\n",
       "      <td>2000-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>19997</td>\n",
       "      <td>Kendra</td>\n",
       "      <td>Bradley</td>\n",
       "      <td>4370 Tammy Turnpike\\nStewartstad, OK 25433</td>\n",
       "      <td>None</td>\n",
       "      <td>Vanessaville</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3311555452</td>\n",
       "      <td>4</td>\n",
       "      <td>2423516578041017</td>\n",
       "      <td>2008/06</td>\n",
       "      <td>kendrabradley123</td>\n",
       "      <td>password</td>\n",
       "      <td>73</td>\n",
       "      <td>60000</td>\n",
       "      <td>M</td>\n",
       "      <td>2000-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>19998</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Webb</td>\n",
       "      <td>02659 Benjamin Mill Suite 976\\nMichellefurt, M...</td>\n",
       "      <td>None</td>\n",
       "      <td>East Johnville</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>0</td>\n",
       "      <td>Russia</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>7635641998</td>\n",
       "      <td>5</td>\n",
       "      <td>8653337796218694</td>\n",
       "      <td>2009/08</td>\n",
       "      <td>robertwebb123</td>\n",
       "      <td>password</td>\n",
       "      <td>36</td>\n",
       "      <td>100000</td>\n",
       "      <td>F</td>\n",
       "      <td>2001-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>19999</td>\n",
       "      <td>Lauren</td>\n",
       "      <td>Stanley</td>\n",
       "      <td>225 Robinson Club Suite 388\\nSouth Staceychest...</td>\n",
       "      <td>None</td>\n",
       "      <td>Port Christina</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>5488070628</td>\n",
       "      <td>4</td>\n",
       "      <td>2519272508565336</td>\n",
       "      <td>2008/12</td>\n",
       "      <td>laurenstanley123</td>\n",
       "      <td>password</td>\n",
       "      <td>28</td>\n",
       "      <td>20000</td>\n",
       "      <td>M</td>\n",
       "      <td>2000-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20001 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       customerid firstname lastname  \\\n",
       "0              11     Becky  Cochran   \n",
       "1              12   Raymond     Yang   \n",
       "2              13   Melanie     Wade   \n",
       "3              14   Heather     Cruz   \n",
       "4              15   Heather  Burgess   \n",
       "...           ...       ...      ...   \n",
       "19996       19995      Marc  Gilbert   \n",
       "19997       19996    Sandra    Ochoa   \n",
       "19998       19997    Kendra  Bradley   \n",
       "19999       19998    Robert     Webb   \n",
       "20000       19999    Lauren  Stanley   \n",
       "\n",
       "                                                address1 address2  \\\n",
       "0                193 Hailey Views\\nMichaelside, AS 48241     None   \n",
       "1           683 Albert Ports\\nLake Waltershire, CO 77913     None   \n",
       "2      514 Tonya Heights Suite 730\\nSouth Davidfurt, ...     None   \n",
       "3      75935 Flynn Island Suite 933\\nSouth Alexis, MD...     None   \n",
       "4      6732 Brandi Trafficway Suite 104\\nNorth Jonath...     None   \n",
       "...                                                  ...      ...   \n",
       "19996  7502 Wilkins Knolls Suite 109\\nSouth Deanna, K...     None   \n",
       "19997     45384 James View Apt. 503\\nNew David, PA 63231     None   \n",
       "19998         4370 Tammy Turnpike\\nStewartstad, OK 25433     None   \n",
       "19999  02659 Benjamin Mill Suite 976\\nMichellefurt, M...     None   \n",
       "20000  225 Robinson Club Suite 388\\nSouth Staceychest...     None   \n",
       "\n",
       "                     city          state    zip    country  region  ...  \\\n",
       "0      East Charleneshire   Pennsylvania  53868         US       1  ...   \n",
       "1                Laneberg   Pennsylvania  18452         US       1  ...   \n",
       "2            Port Jessica       Delaware  53356         US       1  ...   \n",
       "3                 Maryton   North Dakota  44395         US       1  ...   \n",
       "4              South Ryan  New Hampshire  37471         US       1  ...   \n",
       "...                   ...            ...    ...        ...     ...  ...   \n",
       "19996          Proctorton          Texas      0         UK       2  ...   \n",
       "19997         Stevenshire   South Dakota      0      Chile       2  ...   \n",
       "19998        Vanessaville           Ohio      0  Australia       2  ...   \n",
       "19999      East Johnville       Nebraska      0     Russia       2  ...   \n",
       "20000      Port Christina    Connecticut      0         UK       2  ...   \n",
       "\n",
       "            phone creditcardtype        creditcard creditcardexpiration  \\\n",
       "0      2415449050              5  6630987872369588              2010/03   \n",
       "1      1896033667              2  3715867913328111              2011/10   \n",
       "2      3029418206              5  3617457962129265              2009/11   \n",
       "3      3748672054              4  3344003576319665              2011/07   \n",
       "4      3354132892              4  8717996907886119              2008/05   \n",
       "...           ...            ...               ...                  ...   \n",
       "19996  6088190939              1  7026820751838387              2011/08   \n",
       "19997  5392978326              5  9188167935237288              2008/11   \n",
       "19998  3311555452              4  2423516578041017              2008/06   \n",
       "19999  7635641998              5  8653337796218694              2009/08   \n",
       "20000  5488070628              4  2519272508565336              2008/12   \n",
       "\n",
       "                username  password age  income  gender created_at  \n",
       "0        beckycochran123  password  58   60000       M 2002-03-01  \n",
       "1         raymondyang123  password  27   20000       F 2003-10-01  \n",
       "2         melaniewade123  password  43  100000       M 2001-11-01  \n",
       "3         heathercruz123  password  85   80000       M 2003-07-01  \n",
       "4      heatherburgess123  password  66  100000       M 2000-05-01  \n",
       "...                  ...       ...  ..     ...     ...        ...  \n",
       "19996     marcgilbert123  password  20  100000       M 2003-08-01  \n",
       "19997     sandraochoa123  password  78   60000       F 2000-11-01  \n",
       "19998   kendrabradley123  password  73   60000       M 2000-06-01  \n",
       "19999      robertwebb123  password  36  100000       F 2001-08-01  \n",
       "20000   laurenstanley123  password  28   20000       M 2000-12-01  \n",
       "\n",
       "[20001 rows x 21 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "load_staging(data = df_customer.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"customers\", idx_name=\"customerid\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_categories = extract_database(table_name = \"categories\")\n",
    "df_categories.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_categories.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"categories\", idx_name=\"category\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prod_id</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>actor</th>\n",
       "      <th>price</th>\n",
       "      <th>special</th>\n",
       "      <th>common_prod_id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>ACADEMY ACADEMY</td>\n",
       "      <td>PENELOPE GUINESS</td>\n",
       "      <td>25.99</td>\n",
       "      <td>0</td>\n",
       "      <td>1976</td>\n",
       "      <td>2004-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>ACADEMY ACE</td>\n",
       "      <td>EWAN RICKMAN</td>\n",
       "      <td>20.99</td>\n",
       "      <td>0</td>\n",
       "      <td>6289</td>\n",
       "      <td>2004-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>ACADEMY ADAPTATION</td>\n",
       "      <td>VIVIEN KAHN</td>\n",
       "      <td>28.99</td>\n",
       "      <td>0</td>\n",
       "      <td>7173</td>\n",
       "      <td>2004-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>ACADEMY AFFAIR</td>\n",
       "      <td>ALAN MARX</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0</td>\n",
       "      <td>8042</td>\n",
       "      <td>2004-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>ACADEMY AFRICAN</td>\n",
       "      <td>CARRIE HANNAH</td>\n",
       "      <td>11.99</td>\n",
       "      <td>1</td>\n",
       "      <td>2183</td>\n",
       "      <td>2004-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prod_id  category               title             actor  price  special  \\\n",
       "0        1        14     ACADEMY ACADEMY  PENELOPE GUINESS  25.99        0   \n",
       "1        2         6         ACADEMY ACE      EWAN RICKMAN  20.99        0   \n",
       "2        3         6  ACADEMY ADAPTATION       VIVIEN KAHN  28.99        0   \n",
       "3        4         3      ACADEMY AFFAIR         ALAN MARX  14.99        0   \n",
       "4        5         3     ACADEMY AFRICAN     CARRIE HANNAH  11.99        1   \n",
       "\n",
       "   common_prod_id created_at  \n",
       "0            1976 2004-03-01  \n",
       "1            6289 2004-03-01  \n",
       "2            7173 2004-03-01  \n",
       "3            8042 2004-03-01  \n",
       "4            2183 2004-03-01  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract\n",
    "df_products = extract_database(table_name = \"products\")\n",
    "df_products.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "load_staging(data = df_products.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"products\", idx_name=\"prod_id\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_inventory = extract_database(table_name = \"inventory\")\n",
    "df_inventory.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_inventory.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"inventory\", idx_name=\"prod_id\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_orders = extract_database(table_name = \"orders\")\n",
    "df_orders.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_orders.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"orders\", idx_name=\"orderid\",\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Orderlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_orderlines = extract_database(table_name = \"orderlines\")\n",
    "df_orderlines.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_orderlines.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"orderlines\", idx_name=[\"orderid\",\"orderlineid\"],\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Load Data Customer History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_cust_hist = extract_database(table_name = \"cust_hist\")\n",
    "df_cust_hist.head()\n",
    "\n",
    "# Load\n",
    "load_staging(data = df_cust_hist.iloc[:, :-1], schema=\"staging\",\n",
    "             table_name=\"cust_hist\", idx_name=['customerid','orderid','prod_id'],\n",
    "             source=\"database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ingestion Data History API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data API contains data for the period between '2004-01-01' and '2004-02-29'. \n",
    "\n",
    "The API cannot send large amounts of data in a single request, so iterate through the data on a daily basis from '2004-01-01' to '2004-02-29'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Function to create list of date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a list of date strings\n",
    "def date_range(start_date, end_date):\n",
    "    delta = end_date - start_date\n",
    "    return [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(delta.days + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create date range  between '2004-01-01' and '2004-02-29'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range\n",
    "start_date = datetime.strptime(\"2004-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2004-02-29\", \"%Y-%m-%d\")\n",
    "\n",
    "# Generate list of dates\n",
    "dates = date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_api = \"https://api-history-order.vercel.app/api/dummydata\"\n",
    "\n",
    "# Iterate over each day and extract data\n",
    "for date in dates:\n",
    "    list_parameter = {\n",
    "        \"start_date\": date,\n",
    "        \"end_date\": date,\n",
    "    }\n",
    "\n",
    "    # Extract Data\n",
    "    df_backfilling = extract_api(link_api, list_parameter, \"customer_orders_history\")\n",
    "    \n",
    "    #Load Data\n",
    "    if(not df_backfilling.empty):\n",
    "        load_staging(data = df_backfilling, schema=\"staging\",\n",
    "                table_name=\"customer_orders_history\", idx_name=['customer_id','order_id','orderline_id'],\n",
    "                source=\"api\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ingestion Data Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract\n",
    "df_analytic = extract_spreadsheet(worksheet_name = 'dellstore_analytic',\n",
    "                                    key_file = KEY_SPREADSHEET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderid</th>\n",
       "      <th>sum_stock</th>\n",
       "      <th>status</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6114</td>\n",
       "      <td>0</td>\n",
       "      <td>fulfilled</td>\n",
       "      <td>2024-08-05 08:50:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11233</td>\n",
       "      <td>100</td>\n",
       "      <td>backordered</td>\n",
       "      <td>2024-08-05 08:50:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4790</td>\n",
       "      <td>0</td>\n",
       "      <td>fulfilled</td>\n",
       "      <td>2024-08-05 08:50:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>fulfilled</td>\n",
       "      <td>2024-08-05 08:50:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11719</td>\n",
       "      <td>0</td>\n",
       "      <td>fulfilled</td>\n",
       "      <td>2024-08-05 08:50:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 orderid sum_stock       status           created_at\n",
       "1    6114         0    fulfilled  2024-08-05 08:50:45\n",
       "2   11233       100  backordered  2024-08-05 08:50:45\n",
       "3    4790         0    fulfilled  2024-08-05 08:50:45\n",
       "4     273         0    fulfilled  2024-08-05 08:50:45\n",
       "5   11719         0    fulfilled  2024-08-05 08:50:45"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analytic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "load_staging(data = df_analytic, schema=\"staging\",\n",
    "                table_name=\"order_status_analytic\", idx_name=\"orderid\",\n",
    "                source=\"spreadsheet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Structure\n",
    "\n",
    "You can modularize the functions mentioned earlier into separate modules for better management and organization. Here is a suggested structure:\n",
    "\n",
    "```\n",
    "project\n",
    "│   README.md\n",
    "│   .env\n",
    "│   pipeline_staging.py    \n",
    "└───src\n",
    "│   └───log\n",
    "│   |    │   log.py\n",
    "│   └───staging\n",
    "│       └───extract\n",
    "│       │   │   extract_db.py\n",
    "│       │   │   extract_api.py\n",
    "│       │   │   extract_spreadsheet.py\n",
    "│       └───load\n",
    "│       │   │   load_minio.py\n",
    "│       │   │   load_staging.py\n",
    "│       └───models\n",
    "│       │   │   customers.sql\n",
    "│       │   │   products.sql\n",
    "└───creds\n",
    "│   │   data-pipeline.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link git repository: [git repository](https://github.com/Kurikulum-Sekolah-Pacmann/data_pipeline_dellstore.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
